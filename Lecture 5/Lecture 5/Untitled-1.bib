
@article{pathan_sign_2023,
  title     = {Sign language recognition using the fusion of image and hand landmarks through multi-headed convolutional neural network},
  volume    = {13},
  copyright = {2023 The Author(s)},
  issn      = {2045-2322},
  url       = {https://www.nature.com/articles/s41598-023-43852-x},
  doi       = {10.1038/s41598-023-43852-x},
  abstract  = {Sign Language Recognition is a breakthrough for communication among deaf-mute society and has been a critical research topic for years. Although some of the previous studies have successfully recognized sign language, it requires many costly instruments including sensors, devices, and high-end processing power. However, such drawbacks can be easily overcome by employing artificial intelligence-based techniques. Since, in this modern era of advanced mobile technology, using a camera to take video or images is much easier, this study demonstrates a cost-effective technique to detect American Sign Language (ASL) using an image dataset. Here, “Finger Spelling, A” dataset has been used, with 24 letters (except j and z as they contain motion). The main reason for using this dataset is that these images have a complex background with different environments and scene colors. Two layers of image processing have been used: in the first layer, images are processed as a whole for training, and in the second layer, the hand landmarks are extracted. A multi-headed convolutional neural network (CNN) model has been proposed and tested with 30\% of the dataset to train these two layers. To avoid the overfitting problem, data augmentation and dynamic learning rate reduction have been used. With the proposed model, 98.981\% test accuracy has been achieved. It is expected that this study may help to develop an efficient human–machine communication system for a deaf-mute society.},
  language  = {en},
  number    = {1},
  urldate   = {2024-09-07},
  journal   = {Scientific Reports},
  author    = {Pathan, Refat Khan and Biswas, Munmun and Yasmin, Suraiya and Khandaker, Mayeen Uddin and Salman, Mohammad and Youssef, Ahmed A. F.},
  month     = oct,
  year      = {2023},
  keywords  = {Computational science, Image processing},
  pages     = {16975}
}

@misc{moryossef_evaluating_2021,
  title     = {Evaluating the {Immediate} {Applicability} of {Pose} {Estimation} for {Sign} {Language} {Recognition}},
  url       = {http://arxiv.org/abs/2104.10166},
  doi       = {10.48550/arXiv.2104.10166},
  abstract  = {Signed languages are visual languages produced by the movement of the hands, face, and body. In this paper, we evaluate representations based on skeleton poses, as these are explainable, person-independent, privacy-preserving, low-dimensional representations. Basically, skeletal representations generalize over an individual's appearance and background, allowing us to focus on the recognition of motion. But how much information is lost by the skeletal representation? We perform two independent studies using two state-of-the-art pose estimation systems. We analyze the applicability of the pose estimation systems to sign language recognition by evaluating the failure cases of the recognition models. Importantly, this allows us to characterize the current limitations of skeletal pose estimation approaches in sign language recognition.},
  urldate   = {2024-04-16},
  publisher = {arXiv},
  author    = {Moryossef, Amit and Tsochantaridis, Ioannis and Dinn, Joe and Camgöz, Necati Cihan and Bowden, Richard and Jiang, Tao and Rios, Annette and Müller, Mathias and Ebling, Sarah},
  month     = apr,
  year      = {2021},
  note      = {arXiv:2104.10166 [cs]},
  keywords  = {Computer Science - Computation and Language}
}


@article{cao_openpose_2021,
  title        = {{OpenPose}: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields},
  volume       = {43},
  issn         = {0162-8828},
  url          = {https://doi.org/10.1109/TPAMI.2019.2929257},
  doi          = {10.1109/TPAMI.2019.2929257},
  shorttitle   = {{OpenPose}},
  abstract     = {Realtime multi-person 2D pose estimation is a key component in enabling machines to have an understanding of people in images and videos. In this work, we present a realtime approach to detect the 2D pose of multiple people in an image. The proposed method uses a nonparametric representation, which we refer to as Part Affinity Fields ({PAFs}), to learn to associate body parts with individuals in the image. This bottom-up system achieves high accuracy and realtime performance, regardless of the number of people in the image. In previous work, {PAFs} and body part location estimation were refined simultaneously across training stages. We demonstrate that a {PAF}-only refinement rather than both {PAF} and body part location refinement results in a substantial increase in both runtime performance and accuracy. We also present the first combined body and foot keypoint detector, based on an internal annotated foot dataset that we have publicly released. We show that the combined detector not only reduces the inference time compared to running them sequentially, but also maintains the accuracy of each component individually. This work has culminated in the release of {OpenPose}, the first open-source realtime system for multi-person 2D pose detection, including body, foot, hand, and facial keypoints.},
  pages        = {172--186},
  number       = {1},
  journaltitle = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
  author       = {Cao, Zhe and Hidalgo, Gines and Simon, Tomas and Wei, Shih-En and Sheikh, Yaser},
  urldate      = {2024-09-24},
  date         = {2021-01-01}
}



@misc{zhao_best_2023,
  title      = {{BEST}: {BERT} {Pre}-{Training} for {Sign} {Language} {Recognition} with {Coupling} {Tokenization}},
  shorttitle = {{BEST}},
  url        = {http://arxiv.org/abs/2302.05075},
  doi        = {10.48550/arXiv.2302.05075},
  abstract   = {In this work, we are dedicated to leveraging the BERT pre-training success and modeling the domain-specific statistics to fertilize the sign language recognition{\textasciitilde}(SLR) model. Considering the dominance of hand and body in sign language expression, we organize them as pose triplet units and feed them into the Transformer backbone in a frame-wise manner. Pre-training is performed via reconstructing the masked triplet unit from the corrupted input sequence, which learns the hierarchical correlation context cues among internal and external triplet units. Notably, different from the highly semantic word token in BERT, the pose unit is a low-level signal originally located in continuous space, which prevents the direct adoption of the BERT cross-entropy objective. To this end, we bridge this semantic gap via coupling tokenization of the triplet unit. It adaptively extracts the discrete pseudo label from the pose triplet unit, which represents the semantic gesture/body state. After pre-training, we fine-tune the pre-trained encoder on the downstream SLR task, jointly with the newly added task-specific layer. Extensive experiments are conducted to validate the effectiveness of our proposed method, achieving new state-of-the-art performance on all four benchmarks with a notable gain.},
  urldate    = {2024-04-15},
  publisher  = {arXiv},
  author     = {Zhao, Weichao and Hu, Hezhen and Zhou, Wengang and Shi, Jiaxin and Li, Houqiang},
  month      = mar,
  year       = {2023},
  note       = {arXiv:2302.05075 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{kumari_isolated_2024,
  title     = {Isolated {Video}-{Based} {Sign} {Language} {Recognition} {Using} a {Hybrid} {CNN}-{LSTM} {Framework} {Based} on {Attention} {Mechanism}},
  volume    = {13},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  issn      = {2079-9292},
  url       = {https://www.mdpi.com/2079-9292/13/7/1229},
  doi       = {10.3390/electronics13071229},
  abstract  = {Sign language is a complex language that uses hand gestures, body movements, and facial expressions and is majorly used by the deaf community. Sign language recognition (SLR) is a popular research domain as it provides an efficient and reliable solution to bridge the communication gap between people who are hard of hearing and those with good hearing. Recognizing isolated sign language words from video is a challenging research area in computer vision. This paper proposes a hybrid SLR framework that combines a convolutional neural network (CNN) and an attention-based long-short-term memory (LSTM) neural network. We used MobileNetV2 as a backbone model due to its lightweight structure, which reduces the complexity of the model architecture for deriving meaningful features from the video frame sequence. The spatial features are fed to LSTM optimized with an attention mechanism to select the significant gesture cues from the video frames and focus on salient features from the sequential data. The proposed method is evaluated on a benchmark WLASL dataset with 100 classes based on precision, recall, F1-score, and 5-fold cross-validation metrics. Our methodology acquired an average accuracy of 84.65\%. The experiment results illustrate that our model performed effectively and computationally efficiently compared to other state-of-the-art methods.},
  language  = {en},
  number    = {7},
  urldate   = {2024-04-07},
  journal   = {Electronics},
  author    = {Kumari, Diksha and Anand, Radhey Shyam},
  month     = jan,
  year      = {2024},
  keywords  = {CNN, LSTM, attention mechanism, gestures, sign language},
  pages     = {1229}
}

@article{zhang_sign_2021,
  title    = {Sign language recognition based on global-local attention},
  volume   = {80},
  issn     = {1047-3203},
  url      = {https://www.sciencedirect.com/science/article/pii/S1047320321001838},
  doi      = {10.1016/j.jvcir.2021.103280},
  abstract = {Video-level sign language recognition is still a challenging task due to the influence of sign language-independent factors and timing requirements. This paper constructs a sign language recognition framework based on global-local feature description, and proposes a three-dimensional residual global network model with attention layer and a local network model based on target detection. The global feature description is based on the whole video behavior for time series modeling. The improved timing conversion layer is used to explore the timing information of different periods and learn the video representations of different timings. In the local module the hand is located through the target detection network to highlight its key role in the whole sign language behavior, which strengthens the category differences, and compensates the global network. Experiments on two well-known Chinese sign language datasets (SLR\_Dataset and DEVSIGN\_D) show that the proposed method can obtain higher recognition accuracy (respectively 89.2\%, 91\%) and better generalization performance.},
  urldate  = {2024-02-25},
  journal  = {Journal of Visual Communication and Image Representation},
  author   = {Zhang, Shujun and Zhang, Qun},
  month    = oct,
  year     = {2021},
  keywords = {3D convolution network, Global-local attention, Sign language recognition, Time series modeling},
  pages    = {103280}
}

@inproceedings{jain_sign_2023,
  title      = {Sign {Language} {Recognition}: {Current} {State} of {Knowledge} and {Future} {Directions}},
  shorttitle = {Sign {Language} {Recognition}},
  url        = {https://ieeexplore.ieee.org/abstract/document/10307879},
  doi        = {10.1109/ICCCNT56998.2023.10307879},
  abstract   = {A fast-emerging topic, Sign Language Recognition(SLR) focuses on creating technologies that allow computers to interpret sign language. In order to bridge gaps in communication between the hearing and deaf populations and to improve accessibility for those who are deaf, sign language should be acknowledged. These systems can precisely recognise sign language by detecting and tracking hand gestures, face expressions, and body language. The goal of this paper is to provide researchers with a thorough understanding of current SLR technology. To ensure a thorough and accurate analysis, the evaluation includes a number of literature sources, which are academic journals, conferences, and proceedings. The analysis underlines the need for consistent evaluation methodologies and standards and also discusses the datasets and evaluation metrics that are currently available for sign language recognition. Lastly, the report concludes the existing limitations of SLR and offers a roadmap for future possibilities in SLR.},
  urldate    = {2024-02-24},
  booktitle  = {2023 14th {International} {Conference} on {Computing} {Communication} and {Networking} {Technologies} ({ICCCNT})},
  author     = {Jain, Bhawna and Chandna, Manya and Dasgupta, Abantika and Bansal, Kashish},
  month      = jul,
  year       = {2023},
  note       = {ISSN: 2473-7674},
  keywords   = {Benchmark Datasets, Bridges, Computers, Data preprocessing, Deep Learning, Face recognition, Gesture recognition, Knowledge engineering, Machine Learning, Measurement, Sociology},
  pages      = {1--7}
}

@article{vahdani_multi-modal_2023,
  title    = {Multi-{Modal} {Multi}-{Channel} {American} {Sign} {Language} {Recognition}},
  issn     = {2972-3353},
  url      = {https://www.worldscientific.com/doi/abs/10.1142/S2972335324500017},
  doi      = {10.1142/S2972335324500017},
  abstract = {In this paper, we propose a machine learning-based multi-stream framework to recognize American Sign Language (ASL) manual signs and nonmanual gestures (face and head movements) in real time from RGB-D videos. Our approach is based on 3D Convolutional Neural Networks (3D CNNs) by fusing the multi-modal features including hand gestures, facial expressions, and body poses from multiple channels (RGB, Depth, Motion, and Skeleton joints). To learn the overall temporal dynamics in a video, a proxy video is generated by selecting a subset of frames for each video which are then used to train the proposed 3D CNN model. We collected a new ASL dataset, ASL-100-RGBD, which contains 42 RGB-D videos captured by a Microsoft Kinect V2 camera. Each video consists of 100 ASL manual signs, along with RGB channel, Depth maps, Skeleton joints, Face features, and HD face. The dataset is fully annotated for each semantic region (i.e. the time duration of each sign that the human signer performs). Our proposed method achieves 92.88\% accuracy for recognizing 100 ASL sign glosses in our newly collected ASL-100-RGBD dataset. The effectiveness of our framework for recognizing hand gestures from RGB-D videos is further demonstrated on a large-scale dataset, ChaLearn IsoGD, achieving the state-of-the-art results.},
  urldate  = {2024-02-24},
  journal  = {International Journal of Artificial Intelligence and Robotics Research},
  author   = {Vahdani, Elahe and Jing, Longlong and Huenerfauth, Matt and Tian, Yingli},
  month    = nov,
  year     = {2023},
  keywords = {3D Convolutional Neural Networks, American Sign Language recognition, RGB-D video analysis, hand gesture recognition, multi-modality, proxy video},
  pages    = {2450001}
}

@misc{si_skeleton-based_2018,
  title     = {Skeleton-{Based} {Action} {Recognition} with {Spatial} {Reasoning} and {Temporal} {Stack} {Learning}},
  url       = {http://arxiv.org/abs/1805.02335},
  doi       = {10.48550/arXiv.1805.02335},
  abstract  = {Skeleton-based action recognition has made great progress recently, but many problems still remain unsolved. For example, most of the previous methods model the representations of skeleton sequences without abundant spatial structure information and detailed temporal dynamics features. In this paper, we propose a novel model with spatial reasoning and temporal stack learning (SR-TSL) for skeleton based action recognition, which consists of a spatial reasoning network (SRN) and a temporal stack learning network (TSLN). The SRN can capture the high-level spatial structural information within each frame by a residual graph neural network, while the TSLN can model the detailed temporal dynamics of skeleton sequences by a composition of multiple skip-clip LSTMs. During training, we propose a clip-based incremental loss to optimize the model. We perform extensive experiments on the SYSU 3D Human-Object Interaction dataset and NTU RGB+D dataset and verify the effectiveness of each network of our model. The comparison results illustrate that our approach achieves much better results than state-of-the-art methods.},
  urldate   = {2024-02-23},
  publisher = {arXiv},
  author    = {Si, Chenyang and Jing, Ya and Wang, Wei and Wang, Liang and Tan, Tieniu},
  month     = dec,
  year      = {2018},
  note      = {arXiv:1805.02335 [cs]},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{du_hierarchical_2015,
  title     = {Hierarchical recurrent neural network for skeleton based action recognition},
  url       = {https://ieeexplore.ieee.org/document/7298714},
  doi       = {10.1109/CVPR.2015.7298714},
  abstract  = {Human actions can be represented by the trajectories of skeleton joints. Traditional methods generally model the spatial structure and temporal dynamics of human skeleton with hand-crafted features and recognize human actions by well-designed classifiers. In this paper, considering that recurrent neural network (RNN) can model the long-term contextual information of temporal sequences well, we propose an end-to-end hierarchical RNN for skeleton based action recognition. Instead of taking the whole skeleton as the input, we divide the human skeleton into five parts according to human physical structure, and then separately feed them to five subnets. As the number of layers increases, the representations extracted by the subnets are hierarchically fused to be the inputs of higher layers. The final representations of the skeleton sequences are fed into a single-layer perceptron, and the temporally accumulated output of the perceptron is the final decision. We compare with five other deep RNN architectures derived from our model to verify the effectiveness of the proposed network, and also compare with several other methods on three publicly available datasets. Experimental results demonstrate that our model achieves the state-of-the-art performance with high computational efficiency.},
  urldate   = {2024-02-23},
  booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  author    = {Du, Yong and Wang, Wei and Wang, Liang},
  month     = jun,
  year      = {2015},
  note      = {ISSN: 1063-6919},
  keywords  = {Artificial neural networks, Computer architecture, Hidden Markov models, Joints, Neurons, Recurrent neural networks},
  pages     = {1110--1118}
}

@misc{zolfaghari_chained_2017,
  title     = {Chained {Multi}-stream {Networks} {Exploiting} {Pose}, {Motion}, and {Appearance} for {Action} {Classification} and {Detection}},
  url       = {http://arxiv.org/abs/1704.00616},
  doi       = {10.48550/arXiv.1704.00616},
  abstract  = {General human action recognition requires understanding of various visual cues. In this paper, we propose a network architecture that computes and integrates the most important visual cues for action recognition: pose, motion, and the raw images. For the integration, we introduce a Markov chain model which adds cues successively. The resulting approach is efficient and applicable to action classification as well as to spatial and temporal action localization. The two contributions clearly improve the performance over respective baselines. The overall approach achieves state-of-the-art action classification performance on HMDB51, J-HMDB and NTU RGB+D datasets. Moreover, it yields state-of-the-art spatio-temporal action localization results on UCF101 and J-HMDB.},
  urldate   = {2024-02-23},
  publisher = {arXiv},
  author    = {Zolfaghari, Mohammadreza and Oliveira, Gabriel L. and Sedaghat, Nima and Brox, Thomas},
  month     = may,
  year      = {2017},
  note      = {arXiv:1704.00616 [cs]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction, Computer Science - Multimedia, Computer Science - Neural and Evolutionary Computing}
}

@misc{noauthor_papers_nodate,
  title    = {Papers with {Code} - {Deep} {Bilinear} {Learning} for {RGB}-{D} {Action} {Recognition}},
  url      = {https://paperswithcode.com/paper/deep-bilinear-learning-for-rgb-d-action},
  abstract = {No code available yet.},
  language = {en},
  urldate  = {2024-02-23}
}

@inproceedings{choutas_potion_2018,
  title      = {{PoTion}: {Pose} {MoTion} {Representation} for {Action} {Recognition}},
  shorttitle = {{PoTion}},
  url        = {https://ieeexplore.ieee.org/document/8578832},
  doi        = {10.1109/CVPR.2018.00734},
  abstract   = {Most state-of-the-art methods for action recognition rely on a two-stream architecture that processes appearance and motion independently. In this paper, we claim that considering them jointly offers rich information for action recognition. We introduce a novel representation that gracefully encodes the movement of some semantic keypoints. We use the human joints as these keypoints and term our Pose moTion representation PoTion. Specifically, we first run a state-of-the-art human pose estimator [4] and extract heatmaps for the human joints in each frame. We obtain our PoTion representation by temporally aggregating these probability maps. This is achieved by 'colorizing' each of them depending on the relative time of the frames in the video clip and summing them. This fixed-size representation for an entire video clip is suitable to classify actions using a shallow convolutional neural network. Our experimental evaluation shows that PoTion outperforms other state-of-the-art pose representations [6, 48]. Furthermore, it is complementary to standard appearance and motion streams. When combining PoTion with the recent two-stream I3D approach [5], we obtain state-of-the-art performance on the JHMDB, HMDB and UCF101 datasets.},
  urldate    = {2024-02-23},
  booktitle  = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
  author     = {Choutas, Vasileios and Weinzaepfel, Philippe and Revaud, Jérôme and Schmid, Cordelia},
  month      = jun,
  year       = {2018},
  note       = {ISSN: 2575-7075},
  keywords   = {Computer architecture, Heating systems, Image color analysis, Joints, Optical imaging, Standards, Streaming media},
  pages      = {7024--7033}
}

@misc{baradel_human_2017,
  title      = {Human {Action} {Recognition}: {Pose}-based {Attention} draws focus to {Hands}},
  shorttitle = {Human {Action} {Recognition}},
  url        = {http://arxiv.org/abs/1712.08002},
  doi        = {10.48550/arXiv.1712.08002},
  abstract   = {We propose a new spatio-temporal attention based mechanism for human action recognition able to automatically attend to the hands most involved into the studied action and detect the most discriminative moments in an action. Attention is handled in a recurrent manner employing Recurrent Neural Network (RNN) and is fully-differentiable. In contrast to standard soft-attention based mechanisms, our approach does not use the hidden RNN state as input to the attention model. Instead, attention distributions are extracted using external information: human articulated pose. We performed an extensive ablation study to show the strengths of this approach and we particularly studied the conditioning aspect of the attention mechanism. We evaluate the method on the largest currently available human action recognition dataset, NTU-RGB+D, and report state-of-the-art results. Other advantages of our model are certain aspects of explanability, as the spatial and temporal attention distributions at test time allow to study and verify on which parts of the input data the method focuses.},
  urldate    = {2024-02-23},
  publisher  = {arXiv},
  author     = {Baradel, Fabien and Wolf, Christian and Mille, Julien},
  month      = dec,
  year       = {2017},
  note       = {arXiv:1712.08002 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{liu_spatio-temporal_2016,
  title     = {Spatio-{Temporal} {LSTM} with {Trust} {Gates} for {3D} {Human} {Action} {Recognition}},
  url       = {http://arxiv.org/abs/1607.07043},
  doi       = {10.48550/arXiv.1607.07043},
  abstract  = {3D action recognition - analysis of human actions based on 3D skeleton data - becomes popular recently due to its succinctness, robustness, and view-invariant representation. Recent attempts on this problem suggested to develop RNN-based learning methods to model the contextual dependency in the temporal domain. In this paper, we extend this idea to spatio-temporal domains to analyze the hidden sources of action-related information within the input data over both domains concurrently. Inspired by the graphical structure of the human skeleton, we further propose a more powerful tree-structure based traversal method. To handle the noise and occlusion in 3D skeleton data, we introduce new gating mechanism within LSTM to learn the reliability of the sequential input data and accordingly adjust its effect on updating the long-term context information stored in the memory cell. Our method achieves state-of-the-art performance on 4 challenging benchmark datasets for 3D human action analysis.},
  urldate   = {2024-02-23},
  publisher = {arXiv},
  author    = {Liu, Jun and Shahroudy, Amir and Xu, Dong and Wang, Gang},
  month     = jul,
  year      = {2016},
  note      = {arXiv:1607.07043 [cs]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing}
}

@misc{li_independently_2018,
  title      = {Independently {Recurrent} {Neural} {Network} ({IndRNN}): {Building} {A} {Longer} and {Deeper} {RNN}},
  shorttitle = {Independently {Recurrent} {Neural} {Network} ({IndRNN})},
  url        = {http://arxiv.org/abs/1803.04831},
  doi        = {10.48550/arXiv.1803.04831},
  abstract   = {Recurrent neural networks (RNNs) have been widely used for processing sequential data. However, RNNs are commonly difficult to train due to the well-known gradient vanishing and exploding problems and hard to learn long-term patterns. Long short-term memory (LSTM) and gated recurrent unit (GRU) were developed to address these problems, but the use of hyperbolic tangent and the sigmoid action functions results in gradient decay over layers. Consequently, construction of an efficiently trainable deep network is challenging. In addition, all the neurons in an RNN layer are entangled together and their behaviour is hard to interpret. To address these problems, a new type of RNN, referred to as independently recurrent neural network (IndRNN), is proposed in this paper, where neurons in the same layer are independent of each other and they are connected across layers. We have shown that an IndRNN can be easily regulated to prevent the gradient exploding and vanishing problems while allowing the network to learn long-term dependencies. Moreover, an IndRNN can work with non-saturated activation functions such as relu (rectified linear unit) and be still trained robustly. Multiple IndRNNs can be stacked to construct a network that is deeper than the existing RNNs. Experimental results have shown that the proposed IndRNN is able to process very long sequences (over 5000 time steps), can be used to construct very deep networks (21 layers used in the experiment) and still be trained robustly. Better performances have been achieved on various tasks by using IndRNNs compared with the traditional RNN and LSTM. The code is available at https://github.com/Sunnydreamrain/IndRNN\_Theano\_Lasagne.},
  urldate    = {2024-02-23},
  publisher  = {arXiv},
  author     = {Li, Shuai and Li, Wanqing and Cook, Chris and Zhu, Ce and Gao, Yanbo},
  month      = may,
  year       = {2018},
  note       = {arXiv:1803.04831 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning}
}

@misc{li_actional-structural_2019,
  title     = {Actional-{Structural} {Graph} {Convolutional} {Networks} for {Skeleton}-based {Action} {Recognition}},
  url       = {http://arxiv.org/abs/1904.12659},
  doi       = {10.48550/arXiv.1904.12659},
  abstract  = {Action recognition with skeleton data has recently attracted much attention in computer vision. Previous studies are mostly based on fixed skeleton graphs, only capturing local physical dependencies among joints, which may miss implicit joint correlations. To capture richer dependencies, we introduce an encoder-decoder structure, called A-link inference module, to capture action-specific latent dependencies, i.e. actional links, directly from actions. We also extend the existing skeleton graphs to represent higher-order dependencies, i.e. structural links. Combing the two types of links into a generalized skeleton graph, we further propose the actional-structural graph convolution network (AS-GCN), which stacks actional-structural graph convolution and temporal convolution as a basic building block, to learn both spatial and temporal features for action recognition. A future pose prediction head is added in parallel to the recognition head to help capture more detailed action patterns through self-supervision. We validate AS-GCN in action recognition using two skeleton data sets, NTU-RGB+D and Kinetics. The proposed AS-GCN achieves consistently large improvement compared to the state-of-the-art methods. As a side product, AS-GCN also shows promising results for future pose prediction.},
  urldate   = {2024-02-23},
  publisher = {arXiv},
  author    = {Li, Maosen and Chen, Siheng and Chen, Xu and Zhang, Ya and Wang, Yanfeng and Tian, Qi},
  month     = apr,
  year      = {2019},
  note      = {arXiv:1904.12659 [cs]},
  keywords  = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition}
}

@misc{huang_deep_2017,
  title     = {Deep {Learning} on {Lie} {Groups} for {Skeleton}-based {Action} {Recognition}},
  url       = {http://arxiv.org/abs/1612.05877},
  doi       = {10.48550/arXiv.1612.05877},
  abstract  = {In recent years, skeleton-based action recognition has become a popular 3D classification problem. State-of-the-art methods typically first represent each motion sequence as a high-dimensional trajectory on a Lie group with an additional dynamic time warping, and then shallowly learn favorable Lie group features. In this paper we incorporate the Lie group structure into a deep network architecture to learn more appropriate Lie group features for 3D action recognition. Within the network structure, we design rotation mapping layers to transform the input Lie group features into desirable ones, which are aligned better in the temporal domain. To reduce the high feature dimensionality, the architecture is equipped with rotation pooling layers for the elements on the Lie group. Furthermore, we propose a logarithm mapping layer to map the resulting manifold data into a tangent space that facilitates the application of regular output layers for the final classification. Evaluations of the proposed network for standard 3D human action recognition datasets clearly demonstrate its superiority over existing shallow Lie group feature learning methods as well as most conventional deep learning methods.},
  urldate   = {2024-02-23},
  publisher = {arXiv},
  author    = {Huang, Zhiwu and Wan, Chengde and Probst, Thomas and Van Gool, Luc},
  month     = apr,
  year      = {2017},
  note      = {arXiv:1612.05877 [cs]},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{du_hierarchical_2015-1,
  title     = {Hierarchical recurrent neural network for skeleton based action recognition},
  url       = {https://ieeexplore.ieee.org/document/7298714},
  doi       = {10.1109/CVPR.2015.7298714},
  abstract  = {Human actions can be represented by the trajectories of skeleton joints. Traditional methods generally model the spatial structure and temporal dynamics of human skeleton with hand-crafted features and recognize human actions by well-designed classifiers. In this paper, considering that recurrent neural network (RNN) can model the long-term contextual information of temporal sequences well, we propose an end-to-end hierarchical RNN for skeleton based action recognition. Instead of taking the whole skeleton as the input, we divide the human skeleton into five parts according to human physical structure, and then separately feed them to five subnets. As the number of layers increases, the representations extracted by the subnets are hierarchically fused to be the inputs of higher layers. The final representations of the skeleton sequences are fed into a single-layer perceptron, and the temporally accumulated output of the perceptron is the final decision. We compare with five other deep RNN architectures derived from our model to verify the effectiveness of the proposed network, and also compare with several other methods on three publicly available datasets. Experimental results demonstrate that our model achieves the state-of-the-art performance with high computational efficiency.},
  urldate   = {2024-02-23},
  booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  author    = {Du, Yong and Wang, Wei and Wang, Liang},
  month     = jun,
  year      = {2015},
  note      = {ISSN: 1063-6919},
  keywords  = {Artificial neural networks, Computer architecture, Hidden Markov models, Joints, Neurons, Recurrent neural networks},
  pages     = {1110--1118}
}

@misc{cai_jolo-gcn_2020,
  title      = {{JOLO}-{GCN}: {Mining} {Joint}-{Centered} {Light}-{Weight} {Information} for {Skeleton}-{Based} {Action} {Recognition}},
  shorttitle = {{JOLO}-{GCN}},
  url        = {http://arxiv.org/abs/2011.07787},
  doi        = {10.48550/arXiv.2011.07787},
  abstract   = {Skeleton-based action recognition has attracted research attentions in recent years. One common drawback in currently popular skeleton-based human action recognition methods is that the sparse skeleton information alone is not sufficient to fully characterize human motion. This limitation makes several existing methods incapable of correctly classifying action categories which exhibit only subtle motion differences. In this paper, we propose a novel framework for employing human pose skeleton and joint-centered light-weight information jointly in a two-stream graph convolutional network, namely, JOLO-GCN. Specifically, we use Joint-aligned optical Flow Patches (JFP) to capture the local subtle motion around each joint as the pivotal joint-centered visual information. Compared to the pure skeleton-based baseline, this hybrid scheme effectively boosts performance, while keeping the computational and memory overheads low. Experiments on the NTU RGB+D, NTU RGB+D 120, and the Kinetics-Skeleton dataset demonstrate clear accuracy improvements attained by the proposed method over the state-of-the-art skeleton-based methods.},
  urldate    = {2024-02-23},
  publisher  = {arXiv},
  author     = {Cai, Jinmiao and Jiang, Nianjuan and Han, Xiaoguang and Jia, Kui and Lu, Jiangbo},
  month      = nov,
  year       = {2020},
  note       = {arXiv:2011.07787 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{tunga_pose-based_2021,
  title     = {Pose-based sign language recognition using {GCN} and {BERT}},
  url       = {http://openaccess.thecvf.com/content/WACV2021W/HBU/html/Tunga_Pose-Based_Sign_Language_Recognition_Using_GCN_and_BERT_WACVW_2021_paper.html},
  urldate   = {2024-02-23},
  booktitle = {Proceedings of the {IEEE}/{CVF} winter conference on applications of computer vision},
  author    = {Tunga, Anirudh and Nuthalapati, Sai Vidyaranya and Wachs, Juan},
  year      = {2021},
  pages     = {31--40}
}

@article{zhang_multimodal_2019,
  title    = {Multimodal {Spatiotemporal} {Networks} for {Sign} {Language} {Recognition}},
  volume   = {7},
  issn     = {2169-3536},
  url      = {https://ieeexplore.ieee.org/document/8932517/},
  doi      = {10.1109/ACCESS.2019.2959206},
  abstract = {Different from other human behaviors, sign language has the characteristics of limited local motion of upper limb and meticulous hand action. Some sign language gestures are ambiguous in RGB video due to the influence of lighting and background color, which affects the recognition accuracy. We propose a multimodal deep learning architecture for sign language recognition which effectively combines RGB-D input and two-stream spatiotemporal networks. Depth videos, as an effective compensation of RGB input, can supply additional distance information about the signer’s hands. A novel sampling method called ARSS (Aligned Random Sampling in Segments) is put forward to select and align optimal RGB-D video frames, which improves the capacity utilization of multimodal data and reduces the redundancy. We get the hand ROI by joints information of RGB data for local focus in spatial stream. D-shift Net is proposed as depth motion feature extraction in temporal stream, which fully utilizes three dimensional motion information of the sign language. Both streams are fused by convolutional fusion layer to get complementary features. Our approach explored the multimodal information and enhanced the recognition precision. It obtains the state-the-of-art performance on the datasets of CSL (96.7\%) and IsoGD (63.78\%).},
  urldate  = {2024-02-21},
  journal  = {IEEE Access},
  author   = {Zhang, Shujun and Meng, Weijia and Li, Hui and Cui, Xuehong},
  year     = {2019},
  keywords = {Assistive technology, Deep learning, Feature extraction, Gesture recognition, Optical fiber networks, Optical imaging, Sign language recognition, Spatiotemporal phenomena, motion features, multimodal data, two-stream network},
  pages    = {180270--180280}
}

@inproceedings{du_skeleton_2015,
  title     = {Skeleton based action recognition with convolutional neural network},
  url       = {https://ieeexplore.ieee.org/document/7486569},
  doi       = {10.1109/ACPR.2015.7486569},
  abstract  = {Temporal dynamics of postures over time is crucial for sequence-based action recognition. Human actions can be represented by the corresponding motions of articulated skeleton. Most of the existing approaches for skeleton based action recognition model the spatial-temporal evolution of actions based on hand-crafted features. As a kind of hierarchically adaptive filter banks, Convolutional Neural Network (CNN) performs well in representation learning. In this paper, we propose an end-to-end hierarchical architecture for skeleton based action recognition with CNN. Firstly, we represent a skeleton sequence as a matrix by concatenating the joint coordinates in each instant and arranging those vector representations in a chronological order. Then the matrix is quantified into an image and normalized to handle the variable-length problem. The final image is fed into a CNN model for feature extraction and recognition. For the specific structure of such images, the simple max-pooling plays an important role on spatial feature selection as well as temporal frequency adjustment, which can obtain more discriminative joint information for different actions and meanwhile address the variable-frequency problem. Experimental results demonstrate that our method achieves the state-of-art performance with high computational efficiency, especially surpassing the existing result by more than 15 percentage on the challenging ChaLearn gesture recognition dataset.},
  urldate   = {2024-02-23},
  booktitle = {2015 3rd {IAPR} {Asian} {Conference} on {Pattern} {Recognition} ({ACPR})},
  author    = {Du, Yong and Fu, Yun and Wang, Liang},
  month     = nov,
  year      = {2015},
  note      = {ISSN: 2327-0985},
  keywords  = {Adaptation models, Adaptive filters, Hidden Markov models, Joints, Time series analysis, Training},
  pages     = {579--583}
}

@article{hu_global-local_2021,
  title    = {Global-local {Enhancement} {Network} for {NMFs}-aware {Sign} {Language} {Recognition}},
  volume   = {17},
  issn     = {1551-6857, 1551-6865},
  url      = {http://arxiv.org/abs/2008.10428},
  doi      = {10.1145/3436754},
  abstract = {Sign language recognition (SLR) is a challenging problem, involving complex manual features, i.e., hand gestures, and fine-grained non-manual features (NMFs), i.e., facial expression, mouth shapes, etc. Although manual features are dominant, non-manual features also play an important role in the expression of a sign word. Specifically, many sign words convey different meanings due to non-manual features, even though they share the same hand gestures. This ambiguity introduces great challenges in the recognition of sign words. To tackle the above issue, we propose a simple yet effective architecture called Global-local Enhancement Network (GLE-Net), including two mutually promoted streams towards different crucial aspects of SLR. Of the two streams, one captures the global contextual relationship, while the other stream captures the discriminative fine-grained cues. Moreover, due to the lack of datasets explicitly focusing on this kind of features, we introduce the first non-manual-features-aware isolated Chinese sign language dataset{\textasciitilde}(NMFs-CSL) with a total vocabulary size of 1,067 sign words in daily life. Extensive experiments on NMFs-CSL and SLR500 datasets demonstrate the effectiveness of our method.},
  number   = {3},
  urldate  = {2024-02-23},
  journal  = {ACM Transactions on Multimedia Computing, Communications, and Applications},
  author   = {Hu, Hezhen and Zhou, Wengang and Pu, Junfu and Li, Houqiang},
  month    = aug,
  year     = {2021},
  note     = {arXiv:2008.10428 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  pages    = {1--19}
}

@article{deng_imagenet_nodate,
  title    = {{ImageNet}: {A} {Large}-{Scale} {Hierarchical} {Image} {Database}},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a largescale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 5001000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classiﬁcation and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  language = {en},
  author   = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li}
}

@misc{kingma_adam_2017,
  title      = {Adam: {A} {Method} for {Stochastic} {Optimization}},
  shorttitle = {Adam},
  url        = {http://arxiv.org/abs/1412.6980},
  doi        = {10.48550/arXiv.1412.6980},
  abstract   = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  urldate    = {2024-02-23},
  publisher  = {arXiv},
  author     = {Kingma, Diederik P. and Ba, Jimmy},
  month      = jan,
  year       = {2017},
  note       = {arXiv:1412.6980 [cs]},
  keywords   = {Computer Science - Machine Learning}
}

@misc{li_transferring_2020,
  title     = {Transferring {Cross}-domain {Knowledge} for {Video} {Sign} {Language} {Recognition}},
  url       = {http://arxiv.org/abs/2003.03703},
  doi       = {10.48550/arXiv.2003.03703},
  abstract  = {Word-level sign language recognition (WSLR) is a fundamental task in sign language interpretation. It requires models to recognize isolated sign words from videos. However, annotating WSLR data needs expert knowledge, thus limiting WSLR dataset acquisition. On the contrary, there are abundant subtitled sign news videos on the internet. Since these videos have no word-level annotation and exhibit a large domain gap from isolated signs, they cannot be directly used for training WSLR models. We observe that despite the existence of a large domain gap, isolated and news signs share the same visual concepts, such as hand gestures and body movements. Motivated by this observation, we propose a novel method that learns domain-invariant visual concepts and fertilizes WSLR models by transferring knowledge of subtitled news sign to them. To this end, we extract news signs using a base WSLR model, and then design a classifier jointly trained on news and isolated signs to coarsely align these two domain features. In order to learn domain-invariant features within each class and suppress domain-specific features, our method further resorts to an external memory to store the class centroids of the aligned news signs. We then design a temporal attention based on the learnt descriptor to improve recognition performance. Experimental results on standard WSLR datasets show that our method outperforms previous state-of-the-art methods significantly. We also demonstrate the effectiveness of our method on automatically localizing signs from sign news, achieving 28.1 for AP@0.5.},
  urldate   = {2024-02-23},
  publisher = {arXiv},
  author    = {Li, Dongxu and Yu, Xin and Xu, Chenchen and Petersson, Lars and Li, Hongdong},
  month     = mar,
  year      = {2020},
  note      = {arXiv:2003.03703 [cs]},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction, Computer Science - Multimedia}
}

@misc{he_deep_2015,
  title     = {Deep {Residual} {Learning} for {Image} {Recognition}},
  url       = {http://arxiv.org/abs/1512.03385},
  doi       = {10.48550/arXiv.1512.03385},
  abstract  = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  urldate   = {2024-02-23},
  publisher = {arXiv},
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  month     = dec,
  year      = {2015},
  note      = {arXiv:1512.03385 [cs]},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{zhang_chinese_2016,
  title     = {Chinese sign language recognition with adaptive {HMM}},
  url       = {https://ieeexplore.ieee.org/document/7552950},
  doi       = {10.1109/ICME.2016.7552950},
  abstract  = {Sign Language Recognition (SLR) aims at translating the sign language into text or speech, so as to realize the communication between deaf-mute people and ordinary people. This paper proposes a framework based on the Hidden Markov Models (HMMs) benefited from the utilization of the trajectories and hand-shape features of the original sign videos, respectively. First, we propose a new trajectory feature (enhanced shape context), which can capture the spatio-temporal information well. Second, we fetch the hand regions by Kinect mapping functions and describe each frame by HOG (pre-processed by PCA). Moreover, in order to optimize predictions, rather than fixing the number of hidden states for each sign model, we independently determine it through the variation of the hand shapes. As for recognition, we propose a combination method to fuse the probabilities of trajectory and hand shape. At last, we evaluate our approach with our self-building Kinect-based dataset and the experiments demonstrate the effectiveness of our approach.},
  urldate   = {2024-02-23},
  booktitle = {2016 {IEEE} {International} {Conference} on {Multimedia} and {Expo} ({ICME})},
  author    = {Zhang, Jihai and Zhou, Wengang and Xie, Chao and Pu, Junfu and Li, Houqiang},
  month     = jul,
  year      = {2016},
  note      = {ISSN: 1945-788X},
  keywords  = {Adaptation models, Assistive technology, Context, Gesture recognition, Hidden Markov Models, Hidden Markov models, Shape, Sign language recognition, Trajectory, adaptive hidden states, enhanced shape context},
  pages     = {1--6}
}

@article{wang_21d-slr_2022,
  title      = {(2+1){D}-{SLR}: an efficient network for video sign language recognition},
  volume     = {34},
  issn       = {1433-3058},
  shorttitle = {(2+1){D}-{SLR}},
  url        = {https://doi.org/10.1007/s00521-021-06467-9},
  doi        = {10.1007/s00521-021-06467-9},
  abstract   = {The most existing sign language recognition methods have made significant progress. However, there are still problems in the field of sign language recognition: Traditional SLR technology relies on external devices such as data gloves, position tracker, and has achieved limited success. Moreover, the current state-of-the-art vision-based technologies cannot be applied in practice due to the difficulty in balancing accuracy and speed, because most of them pay the cost of running time for better sign language classification accuracy. In this paper, we propose a (2+1)D-SLR network based on (2+1)D convolution, which is different from other methods in that the proposed network can achieve higher accuracy with a faster speed. Because (2+1)D-SLR can learn spatio-temporal features from the raw sign RGB frames. In addition, the existing Chinese sign language dataset is difficult to guarantee the personality differences between different sign language speakers and the presentation differences of the same presenter. Therefore, we propose a large-scale Chinese sign language video dataset called NCSL to solve this problem, including 300 different sign language vocabulary which demonstrated by 30 volunteers, 10 times each. We also validated our method on NCSL and another large-scale sign language dataset, i.e., LSA64, Achieved 96.4\% and 98.7\% accuracy, respectively, demonstrating that our method can not only achieve competitive accuracy but be much faster than current well-known sign language recognition methods.},
  language   = {en},
  number     = {3},
  urldate    = {2024-02-23},
  journal    = {Neural Computing and Applications},
  author     = {Wang, Fei and Du, Yuxuan and Wang, Guorui and Zeng, Zhen and Zhao, Lihong},
  month      = feb,
  year       = {2022},
  keywords   = {Large-scale sign language video dataset, Sign language recognition, Spatio-temporal features, Video understanding},
  pages      = {2413--2423}
}

@misc{szegedy_going_2014,
  title     = {Going {Deeper} with {Convolutions}},
  url       = {http://arxiv.org/abs/1409.4842},
  doi       = {10.48550/arXiv.1409.4842},
  abstract  = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  urldate   = {2024-02-21},
  publisher = {arXiv},
  author    = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  month     = sep,
  year      = {2014},
  note      = {arXiv:1409.4842 [cs]},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition, inceptionnet}
}

@article{singla_feature_2023,
  title    = {Feature {Fusion} and {Multi}-{Stream} {CNNs} for {ScaleAdaptive} {Multimodal} {Sign} {Language} {Recognition}},
  url      = {https://ieeexplore.ieee.org/document/10112915/},
  doi      = {10.1109/ICACCS57279.2023.10112915},
  abstract = {Sign language can be defined as a visual-spatial language used by deaf and hard-of-hearing individuals for everyday communication. However, the use of sign language is often limited by the lack of technology capable of recognizing and interpreting it. Our research aims to create a method for recognizing static sign language, a crucial aspect of sign language communication. We propose using multi-stream Convolutional Neural Networks (CNNs), incorporating local scale awareness and multimodality, to recognize static Hand signs more effectively. The local scale awareness aspect is achieved through spatial pyramidal pooling (SPP), which allows CNN to obtain and merge features from different scales. The multimodal aspect is achieved using Color (RGB), depth information, and extracted image features as these modalities provide complementary information for recognizing SL. We also introduce a method for feature extraction based on the fusion of extracted features from the Gabor Filter and the Local Binary Pattern (LBP) method. The multi-stream CNN architecture is used to effectively combine the information from multiple sources, whereas the proposed feature fusion method provides extensive details about image features, resulting in improved recognition accuracy. We achieved a high accuracy rate in recognizing individual signs and our approach has the potential to improve accessibility for hard-of-hearing communities.},
  urldate  = {2024-02-21},
  journal  = {2023 9th International Conference on Advanced Computing and Communication Systems (ICACCS)},
  author   = {Singla, Navya and Taneja, Manas and Goyal, Neha and Jindal, Rajni},
  month    = mar,
  year     = {2023},
  note     = {Conference Name: 2023 9th International Conference on Advanced Computing and Communication Systems (ICACCS)
              ISBN: 9798350397376
              Place: Coimbatore, India
              Publisher: IEEE},
  pages    = {1266--1273}
}

@article{liang_human_2022,
  title    = {Human gesture recognition of dynamic skeleton using graph convolutional networks},
  volume   = {32},
  issn     = {1017-9909},
  url      = {https://www.spiedigitallibrary.org/journals/journal-of-electronic-imaging/volume-32/issue-02/021402/Human-gesture-recognition-of-dynamic-skeleton-using-graph-convolutional-networks/10.1117/1.JEI.32.2.021402.full},
  doi      = {10.1117/1.JEI.32.2.021402},
  abstract = {Abstract. In this era, intelligent vision computing has always been a fascinating field. With the rapid development in computer vision, dynamic gesture-based recognition systems have attracted significant attention. However, automatically recognizing skeleton-based human gestures in the form of sign language is complex and challenging. Most existing methods consider skeleton-based human gesture recognition as a standard video recognition problem, without considering the rich structure information among both joints and gesture frames. Graph convolutional networks (GCNs) are a promising way to leverage structure information to learn structure representations. However, adopting GCNs to tackle such gesture sequences both in spatial and temporal spaces is challenging as graph could be highly nonlinear and complex. To overcome this issue, we propose the spatiotemporal GCNs model to leverage the powerful spatiotemporal correlations to adaptively construct spatiotemporal graphs, called Aegles. Our method could dynamically attend to relatively significant spatiotemporal joints and construct different graphs, including spatial, temporal, and spatiotemporal graph, and well capturing the structure information in gesture sequences. Besides, we introduce the second-order information of the gesture skeleton data, i.e., the length and orientation of bones, to improve the representation of human hands and fingers. In addition, with the public sign language datasets, we use OpenPose technology to extract human gesture skeleton and obtain human skeleton video, building four skeleton-based sign language recognition datasets. Experimental results show that this Aegles outperforms the state-of-the-art ones and that the spatiotemporal correlations effectively boost the performance of human gesture recognition.},
  number   = {02},
  urldate  = {2024-02-21},
  journal  = {Journal of Electronic Imaging},
  author   = {Liang, Wuyan and Xu, Xiaolong and Xiao, Fu},
  month    = jul,
  year     = {2022}
}

@article{wang_novel_2019,
  title    = {A {Novel} {Sign} {Language} {Recognition} {Framework} {Using} {Hierarchical} {Grassmann} {Covariance} {Matrix}},
  volume   = {21},
  issn     = {1520-9210, 1941-0077},
  url      = {https://ieeexplore.ieee.org/document/8707080/},
  doi      = {10.1109/TMM.2019.2915032},
  abstract = {Visual sign language recognition is an interesting and challenging problem. To create a discriminative representation, a hierarchical Grassmann covariance matrix (HGCM) model is proposed for sign description. Furthermore, a multi-temporal belief propagation (MTBP) based segmentation approach is presented for continuous sequence spotting. Concretely speaking, a sign is represented by multiple covariance matrices, followed by evaluating and selecting their most significant singular vectors. These covariance matrices are transformed into a more compact and discriminative HGCM, which is formulated on the Grassmann manifold. Continuous sign sequences can be recognized frame by frame using the HGCM model, before being optimized by MTBP, which is a carefully designed graphic model. The proposed method is thoroughly evaluated on isolated and synthetic and real continuous sign datasets as well as on HDM05. Extensive experimental results convincingly show the effectiveness of our proposed framework.},
  number   = {11},
  urldate  = {2024-02-21},
  journal  = {IEEE Transactions on Multimedia},
  author   = {Wang, Hanjie and Chai, Xiujuan and Chen, Xilin},
  month    = nov,
  year     = {2019},
  pages    = {2806--2814}
}

@misc{zhang_self-attention_2019,
  title     = {Self-{Attention} {Generative} {Adversarial} {Networks}},
  url       = {http://arxiv.org/abs/1805.08318},
  doi       = {10.48550/arXiv.1805.08318},
  abstract  = {In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.},
  urldate   = {2024-02-21},
  publisher = {arXiv},
  author    = {Zhang, Han and Goodfellow, Ian and Metaxas, Dimitris and Odena, Augustus},
  month     = jun,
  year      = {2019},
  note      = {arXiv:1805.08318 [cs, stat]},
  keywords  = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@misc{huang_video-based_2018,
  title     = {Video-based {Sign} {Language} {Recognition} without {Temporal} {Segmentation}},
  url       = {http://arxiv.org/abs/1801.10111},
  doi       = {10.48550/arXiv.1801.10111},
  abstract  = {Millions of hearing impaired people around the world routinely use some variants of sign languages to communicate, thus the automatic translation of a sign language is meaningful and important. Currently, there are two sub-problems in Sign Language Recognition (SLR), i.e., isolated SLR that recognizes word by word and continuous SLR that translates entire sentences. Existing continuous SLR methods typically utilize isolated SLRs as building blocks, with an extra layer of preprocessing (temporal segmentation) and another layer of post-processing (sentence synthesis). Unfortunately, temporal segmentation itself is non-trivial and inevitably propagates errors into subsequent steps. Worse still, isolated SLR methods typically require strenuous labeling of each word separately in a sentence, severely limiting the amount of attainable training data. To address these challenges, we propose a novel continuous sign recognition framework, the Hierarchical Attention Network with Latent Space (LS-HAN), which eliminates the preprocessing of temporal segmentation. The proposed LS-HAN consists of three components: a two-stream Convolutional Neural Network (CNN) for video feature representation generation, a Latent Space (LS) for semantic gap bridging, and a Hierarchical Attention Network (HAN) for latent space based recognition. Experiments are carried out on two large scale datasets. Experimental results demonstrate the effectiveness of the proposed framework.},
  urldate   = {2024-02-21},
  publisher = {arXiv},
  author    = {Huang, Jie and Zhou, Wengang and Zhang, Qilin and Li, Houqiang and Li, Weiping},
  month     = jan,
  year      = {2018},
  note      = {arXiv:1801.10111 [cs]},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{tran_learning_2015,
  title     = {Learning {Spatiotemporal} {Features} with {3D} {Convolutional} {Networks}},
  url       = {http://arxiv.org/abs/1412.0767},
  doi       = {10.48550/arXiv.1412.0767},
  abstract  = {We propose a simple, yet effective approach for spatiotemporal feature learning using deep 3-dimensional convolutional networks (3D ConvNets) trained on a large scale supervised video dataset. Our findings are three-fold: 1) 3D ConvNets are more suitable for spatiotemporal feature learning compared to 2D ConvNets; 2) A homogeneous architecture with small 3x3x3 convolution kernels in all layers is among the best performing architectures for 3D ConvNets; and 3) Our learned features, namely C3D (Convolutional 3D), with a simple linear classifier outperform state-of-the-art methods on 4 different benchmarks and are comparable with current best methods on the other 2 benchmarks. In addition, the features are compact: achieving 52.8\% accuracy on UCF101 dataset with only 10 dimensions and also very efficient to compute due to the fast inference of ConvNets. Finally, they are conceptually very simple and easy to train and use.},
  urldate   = {2024-02-21},
  publisher = {arXiv},
  author    = {Tran, Du and Bourdev, Lubomir and Fergus, Rob and Torresani, Lorenzo and Paluri, Manohar},
  month     = oct,
  year      = {2015},
  note      = {arXiv:1412.0767 [cs]},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{shen_stepnet_2022,
  title      = {{StepNet}: {Spatial}-temporal {Part}-aware {Network} for {Sign} {Language} {Recognition}},
  shorttitle = {{StepNet}},
  url        = {http://arxiv.org/abs/2212.12857},
  abstract   = {Sign language recognition (SLR) aims to overcome the communication barrier for the people with deafness or the people with hard hearing. Most existing approaches can be typically divided into two lines, i.e., Skeleton-based and RGB-based methods, but both the two lines of methods have their limitations. RGB-based approaches usually overlook the ﬁne-grained hand structure, while Skeleton-based methods do not take the facial expression into account. In attempts to address both limitations, we propose a new framework named Spatial-temporal Part-aware network (StepNet), based on RGB parts. As the name implies, StepNet consists of two modules: Part-level Spatial Modeling and Part-level Temporal Modeling. Particularly, without using any keypoint-level annotations, Partlevel Spatial Modeling implicitly captures the appearance-based properties, such as hands and faces, in the feature space. On the other hand, Part-level Temporal Modeling captures the pertinent properties over time by implicitly mining the longshort term context. Extensive experiments show that our StepNet, thanks to Spatial-temporal modules, achieves competitive Top-1 Per-instance accuracy on three widely-used SLR benchmarks, i.e., 56.89\% on WLASL, 77.2\% on NMFs-CSL, and 77.1\% on BOBSL. Moreover, the proposed method is compatible with the optical ﬂow input, and can yield higher performance if fused. We hope that this work can serve as a preliminary step for the people with deafness.},
  language   = {en},
  urldate    = {2024-02-21},
  publisher  = {arXiv},
  author     = {Shen, Xiaolong and Zheng, Zhedong and Yang, Yi},
  month      = dec,
  year       = {2022},
  note       = {arXiv:2212.12857 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition}
}

@article{sincan_autsl_2020,
  title      = {{AUTSL}: {A} {Large} {Scale} {Multi}-modal {Turkish} {Sign} {Language} {Dataset} and {Baseline} {Methods}},
  volume     = {8},
  issn       = {2169-3536},
  shorttitle = {{AUTSL}},
  url        = {http://arxiv.org/abs/2008.00932},
  doi        = {10.1109/ACCESS.2020.3028072},
  abstract   = {Sign language recognition is a challenging problem where signs are identified by simultaneous local and global articulations of multiple sources, i.e. hand shape and orientation, hand movements, body posture, and facial expressions. Solving this problem computationally for a large vocabulary of signs in real life settings is still a challenge, even with the state-of-the-art models. In this study, we present a new largescale multi-modal Turkish Sign Language dataset (AUTSL) with a benchmark and provide baseline models for performance evaluations. Our dataset consists of 226 signs performed by 43 different signers and 38,336 isolated sign video samples in total. Samples contain a wide variety of backgrounds recorded in indoor and outdoor environments. Moreover, spatial positions and the postures of signers also vary in the recordings. Each sample is recorded with Microsoft Kinect v2 and contains RGB, depth, and skeleton modalities. We prepared benchmark training and test sets for user independent assessments of the models. We trained several deep learning based models and provide empirical evaluations using the benchmark; we used CNNs to extract features, unidirectional and bidirectional LSTM models to characterize temporal information. We also incorporated feature pooling modules and temporal attention to our models to improve the performances. We evaluated our baseline models on AUTSL and Montalbano datasets. Our models achieved competitive results with the state-of-the-art methods on Montalbano dataset, i.e. 96.11\% accuracy. In AUTSL random train-test splits, our models performed up to 95.95\% accuracy. In the proposed user-independent benchmark dataset our best baseline model achieved 62.02\% accuracy. The gaps in the performances of the same baseline models show the challenges inherent in our benchmark dataset. AUTSL benchmark dataset is publicly available at https://cvml.ankara.edu.tr.},
  urldate    = {2024-02-19},
  journal    = {IEEE Access},
  author     = {Sincan, Ozge Mercanoglu and Keles, Hacer Yalim},
  year       = {2020},
  note       = {arXiv:2008.00932 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  pages      = {181340--181355}
}

@article{guo_attention_2022,
  title      = {Attention mechanisms in computer vision: {A} survey},
  volume     = {8},
  issn       = {2096-0433, 2096-0662},
  shorttitle = {Attention mechanisms in computer vision},
  url        = {https://link.springer.com/10.1007/s41095-022-0271-y},
  doi        = {10.1007/s41095-022-0271-y},
  abstract   = {Humans can naturally and eﬀectively ﬁnd salient regions in complex scenes. Motivated by this observation, attention mechanisms were introduced into computer vision with the aim of imitating this aspect of the human visual system. Such an attention mechanism can be regarded as a dynamic weight adjustment process based on features of the input image. Attention mechanisms have achieved great success in many visual tasks, including image classiﬁcation, object detection, semantic segmentation, video understanding, image generation, 3D vision, multimodal tasks, and self-supervised learning. In this survey, we provide a comprehensive review of various attention mechanisms in computer vision and categorize them according to approach, such as channel attention, spatial attention, temporal attention, and branch attention; a related repository https://github.com/MenghaoGuo/ Awesome-Vision-Attentions is dedicated to collecting related work. We also suggest future directions for attention mechanism research.},
  language   = {en},
  number     = {3},
  urldate    = {2024-02-19},
  journal    = {Computational Visual Media},
  author     = {Guo, Meng-Hao and Xu, Tian-Xing and Liu, Jiang-Jiang and Liu, Zheng-Ning and Jiang, Peng-Tao and Mu, Tai-Jiang and Zhang, Song-Hai and Martin, Ralph R. and Cheng, Ming-Ming and Hu, Shi-Min},
  month      = sep,
  year       = {2022},
  pages      = {331--368}
}

@article{de_santana_correia_attention_2022,
  title    = {Attention, please! {A} survey of neural attention models in deep learning},
  volume   = {55},
  issn     = {0269-2821, 1573-7462},
  url      = {https://link.springer.com/10.1007/s10462-022-10148-x},
  doi      = {10.1007/s10462-022-10148-x},
  abstract = {In humans, Attention is a core property of all perceptual and cognitive operations. Given our limited ability to process competing sources, attention mechanisms select, modulate, and focus on the information most relevant to behavior. For decades, concepts and functions of attention have been studied in philosophy, psychology, neuroscience, and computing. For the last 6 years, this property has been widely explored in deep neural networks. Currently, the state-of-the-art in Deep Learning is represented by neural attention models in several application domains. This survey provides a comprehensive overview and analysis of developments in neural attention models. We systematically reviewed hundreds of architectures in the area, identifying and discussing those in which attention has shown a significant impact. We also developed and made public an automated methodology to facilitate the development of reviews in the area. By critically analyzing 650 works, we describe the primary uses of attention in convolutional, recurrent networks, and generative models, identifying common subgroups of uses and applications. Furthermore, we describe the impact of attention in different application domains and their impact on neural networks’ interpretability. Finally, we list possible trends and opportunities for further research, hoping that this review will provide a succinct overview of the main attentional models in the area and guide researchers in developing future approaches that will drive further improvements.},
  language = {en},
  number   = {8},
  urldate  = {2024-02-19},
  journal  = {Artificial Intelligence Review},
  author   = {De Santana Correia, Alana and Colombini, Esther Luna},
  month    = dec,
  year     = {2022},
  pages    = {6037--6124}
}

@inproceedings{martin_3d_2021,
  address   = {Milan, Italy},
  title     = {{3D} attention mechanism for fine-grained classification of table tennis strokes using a {Twin} {Spatio}-{Temporal} {Convolutional} {Neural} {Networks}},
  isbn      = {978-1-72818-808-9},
  url       = {https://ieeexplore.ieee.org/document/9412742/},
  doi       = {10.1109/ICPR48806.2021.9412742},
  abstract  = {The paper addresses the problem of recognition of actions in video with low inter-class variability such as Table Tennis strokes. Two stream, “twin” convolutional neural networks are used with 3D convolutions both on RGB data and optical ﬂow. Actions are recognized by classiﬁcation of temporal windows. We introduce 3D attention modules and examine their impact on classiﬁcation efﬁciency. In the context of the study of sportsmen performances, a corpus of the particular actions of table tennis strokes is considered. The use of attention blocks in the network speeds up the training step and improves the classiﬁcation scores up to 5\% with our twin model. We visualize the impact on the obtained features and notice correlation between attention and player movements and position. Score comparison of state-ofthe-art action classiﬁcation method and proposed approach with attentional blocks is performed on the corpus. Proposed model with attention blocks outperforms previous model without them and our baseline.},
  language  = {en},
  urldate   = {2024-02-19},
  booktitle = {2020 25th {International} {Conference} on {Pattern} {Recognition} ({ICPR})},
  publisher = {IEEE},
  author    = {Martin, Pierre-Etienne and Benois-Pineau, Jenny and Peteri, Renaud and Morlier, Julien},
  month     = jan,
  year      = {2021},
  pages     = {6019--6026}
}

@article{huang_attention-based_2019,
  title    = {Attention-{Based} {3D}-{CNNs} for {Large}-{Vocabulary} {Sign} {Language} {Recognition}},
  volume   = {29},
  issn     = {1051-8215, 1558-2205},
  url      = {https://ieeexplore.ieee.org/document/8466903/},
  doi      = {10.1109/TCSVT.2018.2870740},
  abstract = {Sign language recognition (SLR) is an important and challenging research topic in the multimedia ﬁeld. Conventional techniques for SLR rely on hand-crafted features, which achieve limited success. In this paper, we present an attention based 3D-Convolutional Neural Networks (3D-CNNs) for SLR. The framework has two advantages: 3D convolutional networks learn spatio-temporal features from raw video without prior knowledge, and attention mechanism helps to select the clue. When training 3D-CNN for capturing spatio-temporal features, spatial attention is incorporated into network to focus on the areas of interest. After feature extraction, temporal attention is utilized to select the signiﬁcant motions for classiﬁcation. The proposed method is evaluated on two large scale sign language datasets. The ﬁrst one, collected by ourselves, is a Chinese Sign Language (CSL) dataset that consists of 500 categories. The other is the ChaLearn14 benchmark. The experiment results demonstrate the effectiveness of our approach compared with state-of-the-art algorithms.},
  language = {en},
  number   = {9},
  urldate  = {2024-02-19},
  journal  = {IEEE Transactions on Circuits and Systems for Video Technology},
  author   = {Huang, Jie and Zhou, Wengang and Li, Houqiang and Li, Weiping},
  month    = sep,
  year     = {2019},
  pages    = {2822--2832}
}

@article{agrawal_survey_2016,
  title    = {A survey on manual and non-manual sign language recognition for isolated and continuous sign},
  volume   = {3},
  issn     = {2049-887X},
  url      = {https://www.inderscienceonline.com/doi/abs/10.1504/IJAPR.2016.079048},
  doi      = {10.1504/IJAPR.2016.079048},
  abstract = {Sign language recognition is an important area of human computer interaction (HCI). The last decade witnessed a good number of publications in this field. Furthermore, several surveys can be found in the literature but none of them addresses an overall review in this field. In this paper, we have specifically highlighted the Indian sign language (ISL). The works under the complex and moving background, integration of non-manual signals, large vocabulary and signer independent have got a very little attention in the past. In this paper, we have discussed hand segmentation and tracking, feature extraction and classification methods exist in the literature. Within these methods, we examine the various issues such as signer dependence/independence, manual/non-manual, glove/device-based, vocabulary size, constraints in hand segmentation, and isolated/continuous sign. The purpose of this paper is to provide a complete progress in the field of SLR, specifically in ISL.},
  number   = {2},
  urldate  = {2024-02-21},
  journal  = {International Journal of Applied Pattern Recognition},
  author   = {Agrawal, Subhash Chand and Jalal, Anand Singh and Tripathi, Rajesh Kumar},
  month    = jan,
  year     = {2016},
  note     = {Publisher: Inderscience Publishers},
  keywords = {HCI, Indian sign language, classification, continuous signs, dataset, evaluation measures, feature extraction, hand gestures, hand segmentation, hand tracking, human-computer interaction, isolated signs, manual signals, non-manual signals, sign language recognition, vocabulary size},
  pages    = {99--134}
}

@article{fakhfakh_gesture_2018,
  title    = {Gesture {Recognition} {System} {For} {Isolated} {Word} {Sign} {Language} {Based} {On} key-{Point} {Trajectory} {Matrix}},
  volume   = {22},
  issn     = {1405-5546},
  url      = {http://www.scielo.org.mx/scielo.php?script=sci_abstract&pid=S1405-55462018000401415&lng=es&nrm=iso&tlng=en},
  doi      = {10.13053/cys-22-4-3046},
  language = {en},
  number   = {4},
  urldate  = {2024-02-21},
  journal  = {Computación y Sistemas},
  author   = {Fakhfakh, Sana and Jemaa, Yousra Ben and Fakhfakh, Sana and Jemaa, Yousra Ben},
  month    = dec,
  year     = {2018},
  note     = {Publisher: Instituto Politécnico Nacional, Centro de Investigación en Computación},
  pages    = {1415--1430}
}

@article{voulodimos_deep_2018,
  title      = {Deep {Learning} for {Computer} {Vision}: {A} {Brief} {Review}},
  volume     = {2018},
  issn       = {1687-5265},
  shorttitle = {Deep {Learning} for {Computer} {Vision}},
  url        = {https://www.hindawi.com/journals/cin/2018/7068349/},
  doi        = {10.1155/2018/7068349},
  abstract   = {Over the last years deep learning methods have been shown to outperform previous state-of-the-art machine learning techniques in several fields, with computer vision being one of the most prominent cases. This review paper provides a brief overview of some of the most significant deep learning schemes used in computer vision problems, that is, Convolutional Neural Networks, Deep Boltzmann Machines and Deep Belief Networks, and Stacked Denoising Autoencoders. A brief account of their history, structure, advantages, and limitations is given, followed by a description of their applications in various computer vision tasks, such as object detection, face recognition, action and activity recognition, and human pose estimation. Finally, a brief overview is given of future directions in designing deep learning schemes for computer vision problems and the challenges involved therein.},
  language   = {en},
  urldate    = {2024-02-21},
  journal    = {Computational Intelligence and Neuroscience},
  author     = {Voulodimos, Athanasios and Doulamis, Nikolaos and Doulamis, Anastasios and Protopapadakis, Eftychios},
  month      = feb,
  year       = {2018},
  note       = {Publisher: Hindawi},
  pages      = {e7068349}
}

@inproceedings{perdomo_reconhecimento_2016,
  title     = {Reconhecimento de sinais estáticos de {LIBRAS} com {Support} {Vector} {Machines} usando {Kinect}},
  copyright = {Copyright (c)},
  url       = {https://sol.sbc.org.br/index.php/ctic/article/view/9152},
  abstract  = {This paper presents the author experiments with his own developed prototype aiming computer recognition of the manual alphabet static signs from the Brazilian Sign Language (LIBRAS), captured with Microsoft Kinect depth sensor, using image pattern recognition techniques along with Multiclass Support Vector Machines (SVM) classifiers. The prototype results and an efficiency analysis with execution time measures are presented. The device practical distance limits interval (0.8m to 2.5m) was considered.},
  language  = {pt},
  urldate   = {2024-02-21},
  booktitle = {Anais do {Concurso} de {Trabalhos} de {Iniciação} {Científica} da {SBC} ({CTIC}-{SBC})},
  publisher = {SBC},
  author    = {Perdomo, Leonardo and Siqueira, Mozart Lemos de},
  month     = jul,
  year      = {2016},
  pages     = {61--70}
}

@misc{noauthor_sign_nodate,
  title   = {Sign {Language} {Recognition} using {Sequential} {Pattern} {Trees} - {University} of {Surrey}},
  url     = {https://openresearch.surrey.ac.uk/esploro/outputs/conferencePresentation/Sign-Language-Recognition-using-Sequential-Pattern/99511676702346},
  urldate = {2024-02-21}
}

@inproceedings{starner_real-time_1995,
  title     = {Real-time {American} {Sign} {Language} recognition from video using hidden {Markov} models},
  url       = {https://ieeexplore.ieee.org/document/477012},
  doi       = {10.1109/ISCV.1995.477012},
  abstract  = {Hidden Markov models (HMMs) have been used prominently and successfully in speech recognition and, more recently, in handwriting recognition. Consequently, they seem ideal for visual recognition of complex, structured hand gestures such as are found in sign language. We describe a real-time HMM-based system for recognizing sentence level American Sign Language (ASL) which attains a word accuracy of 99.2\% without explicitly modeling the fingers.},
  urldate   = {2024-02-21},
  booktitle = {Proceedings of {International} {Symposium} on {Computer} {Vision} - {ISCV}},
  author    = {Starner, T. and Pentland, A.},
  month     = nov,
  year      = {1995},
  keywords  = {Face recognition, Fingers, Handicapped aids, Handwriting recognition, Hidden Markov models, Laboratories, Natural languages, Real time systems, Shape, Speech recognition},
  pages     = {265--270}
}

@article{madhiarasan_comprehensive_2022,
  title      = {A {Comprehensive} {Review} of {Sign} {Language} {Recognition}: {Different} {Types}, {Modalities}, and {Datasets}},
  copyright  = {Creative Commons Attribution 4.0 International},
  shorttitle = {A {Comprehensive} {Review} of {Sign} {Language} {Recognition}},
  url        = {https://arxiv.org/abs/2204.03328},
  doi        = {10.48550/ARXIV.2204.03328},
  abstract   = {A machine can understand human activities, and the meaning of signs can help overcome the communication barriers between the inaudible and ordinary people. Sign Language Recognition (SLR) is a fascinating research area and a crucial task concerning computer vision and pattern recognition. Recently, SLR usage has increased in many applications, but the environment, background image resolution, modalities, and datasets affect the performance a lot. Many researchers have been striving to carry out generic real-time SLR models. This review paper facilitates a comprehensive overview of SLR and discusses the needs, challenges, and problems associated with SLR. We study related works about manual and non-manual, various modalities, and datasets. Research progress and existing state-of-the-art SLR models over the past decade have been reviewed. Finally, we find the research gap and limitations in this domain and suggest future directions. This review paper will be helpful for readers and researchers to get complete guidance about SLR and the progressive design of the state-of-the-art SLR model},
  urldate    = {2024-02-21},
  author     = {Madhiarasan, Dr. M. and Roy, Prof. Partha Pratim},
  year       = {2022},
  note       = {Publisher: arXiv
                Version Number: 1},
  keywords   = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences}
}

@article{rastgoo_sign_2021,
  title      = {Sign {Language} {Recognition}: {A} {Deep} {Survey}},
  volume     = {164},
  issn       = {09574174},
  shorttitle = {Sign {Language} {Recognition}},
  url        = {https://linkinghub.elsevier.com/retrieve/pii/S095741742030614X},
  doi        = {10.1016/j.eswa.2020.113794},
  abstract   = {Sign language, as a different form of the communication language, is important to large groups of people in society. There are different signs in each sign language with variability in hand shape, motion profile, and position of the hand, face, and body parts contributing to each sign. So, visual sign language recognition is a complex research area in computer vision. Many models have been proposed by different researchers with significant improvement by deep learning approaches in recent years. In this survey, we review the visionbased proposed models of sign language recognition using deep learning approaches from the last five years. While the overall trend of the proposed models indicates a significant improvement in recognition accuracy in sign language recognition, there are some challenges yet that need to be solved. We present a taxonomy to categorize the proposed models for isolated and continuous sign language recognition, discussing applications, datasets, hybrid models, complexity, and future lines of research in the field.},
  language   = {en},
  urldate    = {2024-02-21},
  journal    = {Expert Systems with Applications},
  author     = {Rastgoo, Razieh and Kiani, Kourosh and Escalera, Sergio},
  month      = feb,
  year       = {2021},
  pages      = {113794}
}

@misc{qiu_learning_2017,
  title     = {Learning {Spatio}-{Temporal} {Representation} with {Pseudo}-{3D} {Residual} {Networks}},
  url       = {http://arxiv.org/abs/1711.10305},
  doi       = {10.48550/arXiv.1711.10305},
  abstract  = {Convolutional Neural Networks (CNN) have been regarded as a powerful class of models for image recognition problems. Nevertheless, it is not trivial when utilizing a CNN for learning spatio-temporal video representation. A few studies have shown that performing 3D convolutions is a rewarding approach to capture both spatial and temporal dimensions in videos. However, the development of a very deep 3D CNN from scratch results in expensive computational cost and memory demand. A valid question is why not recycle off-the-shelf 2D networks for a 3D CNN. In this paper, we devise multiple variants of bottleneck building blocks in a residual learning framework by simulating \$3{\textbackslash}times3{\textbackslash}times3\$ convolutions with \$1{\textbackslash}times3{\textbackslash}times3\$ convolutional filters on spatial domain (equivalent to 2D CNN) plus \$3{\textbackslash}times1{\textbackslash}times1\$ convolutions to construct temporal connections on adjacent feature maps in time. Furthermore, we propose a new architecture, named Pseudo-3D Residual Net (P3D ResNet), that exploits all the variants of blocks but composes each in different placement of ResNet, following the philosophy that enhancing structural diversity with going deep could improve the power of neural networks. Our P3D ResNet achieves clear improvements on Sports-1M video classification dataset against 3D CNN and frame-based 2D CNN by 5.3\% and 1.8\%, respectively. We further examine the generalization performance of video representation produced by our pre-trained P3D ResNet on five different benchmarks and three different tasks, demonstrating superior performances over several state-of-the-art techniques.},
  urldate   = {2024-02-20},
  publisher = {arXiv},
  author    = {Qiu, Zhaofan and Yao, Ting and Mei, Tao},
  month     = nov,
  year      = {2017},
  note      = {arXiv:1711.10305 [cs]},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition, P3D}
}

@misc{feichtenhofer_x3d_2020,
  title      = {{X3D}: {Expanding} {Architectures} for {Efficient} {Video} {Recognition}},
  shorttitle = {{X3D}},
  url        = {http://arxiv.org/abs/2004.04730},
  doi        = {10.48550/arXiv.2004.04730},
  abstract   = {This paper presents X3D, a family of efficient video networks that progressively expand a tiny 2D image classification architecture along multiple network axes, in space, time, width and depth. Inspired by feature selection methods in machine learning, a simple stepwise network expansion approach is employed that expands a single axis in each step, such that good accuracy to complexity trade-off is achieved. To expand X3D to a specific target complexity, we perform progressive forward expansion followed by backward contraction. X3D achieves state-of-the-art performance while requiring 4.8x and 5.5x fewer multiply-adds and parameters for similar accuracy as previous work. Our most surprising finding is that networks with high spatiotemporal resolution can perform well, while being extremely light in terms of network width and parameters. We report competitive accuracy at unprecedented efficiency on video classification and detection benchmarks. Code will be available at: https://github.com/facebookresearch/SlowFast},
  urldate    = {2024-02-20},
  publisher  = {arXiv},
  author     = {Feichtenhofer, Christoph},
  month      = apr,
  year       = {2020},
  note       = {arXiv:2004.04730 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{amrutha_ml_2021,
  address   = {Kottayam, India},
  title     = {{ML} {Based} {Sign} {Language} {Recognition} {System}},
  isbn      = {978-1-66540-467-9},
  url       = {https://ieeexplore.ieee.org/document/9399594/},
  doi       = {10.1109/ICITIIT51526.2021.9399594},
  abstract  = {This paper reviews different steps in an automated sign language recognition (SLR) system. Developing a system that can read and interpret a sign must be trained using a large dataset and the best algorithm. As a basic SLR system, an isolated recognition model is developed. The model is based on vision-based isolated hand gesture detection and recognition. Assessment of ML-based SLR model was conducted with the help of 4 candidates under a controlled environment. The model made use of a convex hull for feature extraction and KNN for classification. The model yielded 65\% accuracy.},
  language  = {en},
  urldate   = {2024-02-20},
  booktitle = {2021 {International} {Conference} on {Innovative} {Trends} in {Information} {Technology} ({ICITIIT})},
  publisher = {IEEE},
  author    = {Amrutha, K and Prabu, P},
  month     = feb,
  year      = {2021},
  pages     = {1--6}
}

@misc{noauthor_deep_nodate,
  title   = {Deep {Learning} for {Sign} {Language} {Recognition}: {Current} {Techniques}, {Benchmarks}, and {Open} {Issues} {\textbar} {IEEE} {Journals} \& {Magazine} {\textbar} {IEEE} {Xplore}},
  url     = {https://ieeexplore.ieee.org/abstract/document/9530569},
  urldate = {2024-02-20}
}

@article{wadhawan_sign_2021,
  title      = {Sign {Language} {Recognition} {Systems}: {A} {Decade} {Systematic} {Literature} {Review}},
  volume     = {28},
  issn       = {1886-1784},
  shorttitle = {Sign {Language} {Recognition} {Systems}},
  url        = {https://doi.org/10.1007/s11831-019-09384-2},
  doi        = {10.1007/s11831-019-09384-2},
  abstract   = {Despite the importance of sign language recognition systems, there is a lack of a Systematic Literature Review and a classification scheme for it. This is the first identifiable academic literature review of sign language recognition systems. It provides an academic database of literature between the duration of 2007–2017 and proposes a classification scheme to classify the research articles. Three hundred and ninety six research articles were identified and reviewed for their direct relevance to sign language recognition systems. One hundred and seventeen research articles were subsequently selected, reviewed and classified. Each of 117 selected papers was categorized on the basis of twenty five sign languages and were further compared on the basis of six dimensions (data acquisition techniques, static/dynamic signs, signing mode, single/double handed signs, classification technique and recognition rate). The Systematic Literature Review and classification process was verified independently. Literature findings of this paper indicate that the major research on sign language recognition has been performed on static, isolated and single handed signs using camera. Overall, it will be hoped that the study may provide readers and researchers a roadmap to guide future research and facilitate knowledge accumulation and creation into the field of sign language recognition.},
  language   = {en},
  number     = {3},
  urldate    = {2024-02-20},
  journal    = {Archives of Computational Methods in Engineering},
  author     = {Wadhawan, Ankita and Kumar, Parteek},
  month      = may,
  year       = {2021},
  pages      = {785--813}
}

@misc{noauthor_cnn_nodate,
  title   = {{CNN} and {Stacked} {LSTM} {Model} for {Indian} {Sign} {Language} {Recognition} {\textbar} {Semantic} {Scholar}},
  url     = {https://www.semanticscholar.org/paper/CNN-and-Stacked-LSTM-Model-for-Indian-Sign-Language-Aparna-Geetha/31e4e5a152409c2ea4553c3643e15315315f39dd},
  urldate = {2024-02-20}
}

@article{mittal_modified_2019,
  title    = {A {Modified} {LSTM} {Model} for {Continuous} {Sign} {Language} {Recognition} {Using} {Leap} {Motion}},
  volume   = {19},
  issn     = {1530-437X},
  url      = {https://journals.scholarsportal.info/details/1530437x/v19i0016/7056_amlmfcslrulm.xml},
  doi      = {10.1109/JSEN.2019.2909837},
  abstract = {Sign language facilitates communication between hearing impaired peoples and the rest of the society. A number of sign language recognition (SLR) systems have been developed by researchers, but they are limited to isolated sign gestures only. In this paper, we propose a modified long short-term memory (LSTM) model for continuous sequences of gestures or continuous SLR that recognizes a sequence of connected gestures. It is based on splitting of continuous signs into sub-units and modeling them with neural networks. Thus, the consideration of a different combination of sub-units is not required during training. The proposed system has been tested with 942 signed sentences of Indian Sign Language (ISL). These sign sentences are recognized using 35 different sign words. The average accuracy of 72.3\% and 89.5\% has been recorded on signed sentences and isolated sign words, respectively.},
  number   = {16},
  urldate  = {2024-02-20},
  journal  = {IEEE Sensors Journal},
  author   = {Mittal, Anshul and Kumar, Pradeep and Roy, Partha Pratim and Balasubramanian, Raman and Chaudhuri, Bidyut B.},
  year     = {2019},
  note     = {Publisher: IEEE},
  keywords = {Assistive technology, Cameras, Feature extraction, Gesture recognition, Hidden Markov models, Indian Sign Language, Sensors, Sign language recognition, Three-dimensional displays, connected gestures, continuous SLR, continuous sign language recognition, continuous signs, deep neural networks, depth sensors, handicapped aids, hearing impaired people communication, isolated sign gestures, leap motion, leap motion sensor, learning (artificial intelligence), modified LSTM model, modified long short-term memory model, natural language processing, neural networks, recurrent neural nets, sign language recognition, sign language recognition systems, sign sentences, sign words, signed sentences},
  pages    = {7056--7063}
}

@article{rastgoo_real-time_2022,
  title    = {Real-time isolated hand sign language recognition using deep networks and {SVD}},
  volume   = {13},
  issn     = {1868-5145},
  url      = {https://doi.org/10.1007/s12652-021-02920-8},
  doi      = {10.1007/s12652-021-02920-8},
  abstract = {One of the challenges in computer vision models, especially sign language, is real-time recognition. In this work, we present a simple yet low-complex and efficient model, comprising single shot detector, 2D convolutional neural network, singular value decomposition (SVD), and long short term memory, to real-time isolated hand sign language recognition (IHSLR) from RGB video. We employ the SVD method as an efficient, compact, and discriminative feature extractor from the estimated 3D hand keypoints coordinators. Despite the previous works that employ the estimated 3D hand keypoints coordinates as raw features, we propose a novel and revolutionary way to apply the SVD to the estimated 3D hand keypoints coordinates to get more discriminative features. SVD method is also applied to the geometric relations between the consecutive segments of each finger in each hand and also the angles between these sections. We perform a detailed analysis of recognition time and accuracy. One of our contributions is that this is the first time that the SVD method is applied to the hand pose parameters. Results on four datasets, RKS-PERSIANSIGN (\$\$99.5 {\textbackslash}pm 0.04\$\$), First-Person (\$\$91 {\textbackslash}pm 0.06\$\$), ASVID (\$\$93 {\textbackslash}pm 0.05\$\$), and isoGD (\$\$86.1 {\textbackslash}pm 0.04\$\$), confirm the efficiency of our method in both accuracy (\$\$mean + std\$\$) and time recognition. Furthermore, our model outperforms or gets competitive results with the state-of-the-art alternatives in IHSLR and hand action recognition.},
  language = {en},
  number   = {1},
  urldate  = {2024-02-19},
  journal  = {Journal of Ambient Intelligence and Humanized Computing},
  author   = {Rastgoo, Razieh and Kiani, Kourosh and Escalera, Sergio},
  month    = jan,
  year     = {2022},
  keywords = {Hand pose estimation, Isolated hand sign language recognition, RGB video, Real-time, Singular value decomposition (SVD)},
  pages    = {591--611}
}

@misc{lugaresi_mediapipe_2019,
  title      = {{MediaPipe}: {A} {Framework} for {Building} {Perception} {Pipelines}},
  shorttitle = {{MediaPipe}},
  url        = {http://arxiv.org/abs/1906.08172},
  abstract   = {Building applications that perceive the world around them is challenging. A developer needs to (a) select and develop corresponding machine learning algorithms and models, (b) build a series of prototypes and demos, (c) balance resource consumption against the quality of the solutions, and ﬁnally (d) identify and mitigate problematic cases. The MediaPipe framework addresses all of these challenges. A developer can use MediaPipe to build prototypes by combining existing perception components, to advance them to polished cross-platform applications and measure system performance and resource consumption on target platforms. We show that these features enable a developer to focus on the algorithm or model development and use MediaPipe as an environment for iteratively improving their application with results reproducible across different devices and platforms. MediaPipe will be open-sourced at https://github.com/google/mediapipe.},
  language   = {en},
  urldate    = {2024-02-19},
  publisher  = {arXiv},
  author     = {Lugaresi, Camillo and Tang, Jiuqiang and Nash, Hadon and McClanahan, Chris and Uboweja, Esha and Hays, Michael and Zhang, Fan and Chang, Chuo-Ling and Yong, Ming Guang and Lee, Juhyun and Chang, Wan-Teh and Hua, Wei and Georg, Manfred and Grundmann, Matthias},
  month      = jun,
  year       = {2019},
  note       = {arXiv:1906.08172 [cs]},
  keywords   = {Computer Science - Distributed, Parallel, and Cluster Computing}
}

@article{vedaldi_bsl-1k_2020,
  title      = {{BSL}-{1K}: {Scaling} {Up} {Co}-articulated {Sign} {Language} {Recognition} {Using} {Mouthing} {Cues}},
  volume     = {12356},
  shorttitle = {{BSL}-{1K}},
  url        = {https://link.springer.com/10.1007/978-3-030-58621-8_3},
  doi        = {10.1007/978-3-030-58621-8_3},
  abstract   = {Recent progress in fine-grained gesture and action classification, and machine translation, point to the possibility of automated sign language recognition becoming a reality. A key stumbling block in making progress towards this goal is a lack of appropriate training data, stemming from the high complexity of sign annotation and a limited supply of qualified annotators. In this work, we introduce a new scalable approach to data collection for sign recognition in continuous videos. We make use of weakly-aligned subtitles for broadcast footage together with a keyword spotting method to automatically localise sign-instances for a vocabulary of 1,000 signs in 1,000 hours of video. We make the following contributions: (1) We show how to use mouthing cues from signers to obtain high-quality annotations from video data - the result is the BSL-1K dataset, a collection of British Sign Language (BSL) signs of unprecedented scale; (2) We show that we can use BSL-1K to train strong sign recognition models for co-articulated signs in BSL and that these models additionally form excellent pretraining for other sign languages and benchmarks - we exceed the state of the art on both the MSASL and WLASL benchmarks. Finally, (3) we propose new large-scale evaluation sets for the tasks of sign recognition and sign spotting and provide baselines which we hope will serve to stimulate research in this area.},
  language   = {en},
  urldate    = {2024-02-19},
  author     = {Albanie, Samuel and Varol, Gül and Momeni, Liliane and Afouras, Triantafyllos and Chung, Joon Son and Fox, Neil and Zisserman, Andrew},
  editor     = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
  year       = {2020},
  doi        = {10.1007/978-3-030-58621-8_3},
  note       = {Book Title: Computer Vision – ECCV 2020
                ISBN: 9783030586201 9783030586218
                Place: Cham
                Publisher: Springer International Publishing},
  pages      = {35--53}
}

@article{varol_read_2021,
  title      = {Read and {Attend}: {Temporal} {Localisation} in {Sign} {Language} {Videos}},
  shorttitle = {Read and {Attend}},
  url        = {https://ieeexplore.ieee.org/document/9578762/},
  doi        = {10.1109/CVPR46437.2021.01658},
  abstract   = {The objective of this work is to annotate sign instances across a broad vocabulary in continuous sign language. We train a Transformer model to ingest a continuous signing stream and output a sequence of written tokens on a large-scale collection of signing footage with weakly-aligned subtitles. We show that through this training it acquires the ability to attend to a large vocabulary of sign instances in the input sequence, enabling their localisation. Our contributions are as follows: (1) we demonstrate the ability to leverage large quantities of continuous signing videos with weakly-aligned subtitles to localise signs in continuous sign language; (2) we employ the learned attention to automatically generate hundreds of thousands of annotations for a large sign vocabulary; (3) we collect a set of 37K manually verified sign instances across a vocabulary of 950 sign classes to support our study of sign language recognition; (4) by training on the newly annotated data from our method, we outperform the prior state of the art on the BSL-1K sign language recognition benchmark.},
  urldate    = {2024-02-19},
  journal    = {2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  author     = {Varol, Gul and Momeni, Liliane and Albanie, Samuel and Afouras, Triantafyllos and Zisserman, Andrew},
  month      = jun,
  year       = {2021},
  note       = {Conference Name: 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
                ISBN: 9781665445092
                Place: Nashville, TN, USA
                Publisher: IEEE},
  pages      = {16852--16861}
}

@article{li_transferring_2020-1,
  title    = {Transferring {Cross}-{Domain} {Knowledge} for {Video} {Sign} {Language} {Recognition}},
  url      = {https://ieeexplore.ieee.org/document/9157542/},
  doi      = {10.1109/CVPR42600.2020.00624},
  abstract = {Word-level sign language recognition (WSLR) is a fundamental task in sign language interpretation. It requires models to recognize isolated sign words from videos. However, annotating WSLR data needs expert knowledge, thus limiting WSLR dataset acquisition. On the contrary, there are abundant subtitled sign news videos on the internet. Since these videos have no word-level annotation and exhibit a large domain gap from isolated signs, they cannot be directly used for training WSLR models. We observe that despite the existence of a large domain gap, isolated and news signs share the same visual concepts, such as hand gestures and body movements. Motivated by this observation, we propose a novel method that learns domain-invariant visual concepts and fertilizes WSLR models by transferring knowledge of subtitled news sign to them. To this end, we extract news signs using a base WSLR model, and then design a classifier jointly trained on news and isolated signs to coarsely align these two domain features. In order to learn domain-invariant features within each class and suppress domain-specific features, our method further resorts to an external memory to store the class centroids of the aligned news signs. We then design a temporal attention based on the learnt descriptor to improve recognition performance. Experimental results on standard WSLR datasets show that our method outperforms previous state-of-the-art methods significantly. We also demonstrate the effectiveness of our method on automatically localizing signs from sign news, achieving 28.1 for AP@0.5.},
  urldate  = {2024-02-19},
  journal  = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  author   = {Li, Dongxu and Yu, Xin and Xu, Chenchen and Petersson, Lars and Li, Hongdong},
  month    = jun,
  year     = {2020},
  note     = {Conference Name: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
              ISBN: 9781728171685
              Place: Seattle, WA, USA
              Publisher: IEEE},
  pages    = {6204--6213}
}

@article{joze_ms-asl_2018,
  title      = {{MS}-{ASL}: {A} {Large}-{Scale} {Data} {Set} and {Benchmark} for {Understanding} {American} {Sign} {Language}},
  shorttitle = {{MS}-{ASL}},
  url        = {https://www.semanticscholar.org/paper/MS-ASL%3A-A-Large-Scale-Data-Set-and-Benchmark-for-Joze-Koller/310a8e2c9f2650fa2e44fdf0d82d11c0cb3e387e},
  abstract   = {Sign language recognition is a challenging and often underestimated problem comprising multi-modal articulators (handshape, orientation, movement, upper body and face) that integrate asynchronously on multiple streams. Learning powerful statistical models in such a scenario requires much data, particularly to apply recent advances of the field. However, labeled data is a scarce resource for sign language due to the enormous cost of transcribing these unwritten languages. 
                We propose the first real-life large-scale sign language data set comprising over 25,000 annotated videos, which we thoroughly evaluate with state-of-the-art methods from sign and related action recognition. Unlike the current state-of-the-art, the data set allows to investigate the generalization to unseen individuals (signer-independent test) in a realistic setting with over 200 signers. Previous work mostly deals with limited vocabulary tasks, while here, we cover a large class count of 1000 signs in challenging and unconstrained real-life recording conditions. We further propose I3D, known from video classifications, as a powerful and suitable architecture for sign language recognition, outperforming the current state-of-the-art by a large margin. The data set is publicly available to the community.},
  urldate    = {2024-02-19},
  journal    = {ArXiv},
  author     = {Joze, Hamid Reza Vaezi and Koller, Oscar},
  month      = dec,
  year       = {2018}
}

@article{papadimitriou_sign_2023,
  title    = {Sign {Language} {Recognition} via {Deformable} {3D} {Convolutions} and {Modulated} {Graph} {Convolutional} {Networks}},
  url      = {https://ieeexplore.ieee.org/document/10096714/},
  doi      = {10.1109/ICASSP49357.2023.10096714},
  abstract = {Automatic sign language recognition (SLR) remains challenging, especially when employing RGB video alone (i.e., with no depth or special glove-based input) and under a signer-independent (SI) framework, due to inter-personal signing variation. In this paper, we address SI isolated SLR from RGB video, proposing an innovative deep-learning framework that leverages multi-modal appearanceand skeleton-based information. Specifically, we propose three components for the first time in SLR: (i) a modified version of the ResNet2+1D network to capture signing appearance information, where spatial and temporal convolutions are substituted by their deformable counterparts, accomplishing both prevalent spatial modeling potential and motion-aware modeling adaptability; (ii) a novel spatio-temporal graph convolutional network (ST-GCN) that integrates a GCN variant, involving weight and affinity modulation for modeling diverse correlations between different body joints beyond the physical human skeleton structure, followed by a self-attention layer and a temporal convolution; and (iii) the “PIXIE” 3D human pose and shape regressor to generate 3D joint-rotation parameterization used for ST-GCN graph construction. Both appearance- and skeleton-based streams are ensembled in the proposed system and evaluated on two datasets of isolated signs, one in Turkish and one in Greek. Our system outperforms the state-of-the-art on the second set, yielding 53\% relative error rate reduction (2.45\% absolute), while it performs on par with the best reported system on the first.},
  urldate  = {2024-02-19},
  journal  = {ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  author   = {Papadimitriou, Katerina and Potamianos, Gerasimos},
  month    = jun,
  year     = {2023},
  note     = {Conference Name: ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
              ISBN: 9781728163277
              Place: Rhodes Island, Greece
              Publisher: IEEE},
  pages    = {1--5}
}

@article{ozdemir_multi-cue_2023,
  title    = {Multi-cue temporal modeling for skeleton-based sign language recognition},
  volume   = {17},
  issn     = {1662-453X},
  url      = {https://www.frontiersin.org/articles/10.3389/fnins.2023.1148191/full},
  doi      = {10.3389/fnins.2023.1148191},
  abstract = {Sign languages are visual languages used as the primary communication medium for the Deaf community. The signs comprise manual and non-manual articulators such as hand shapes, upper body movement, and facial expressions. Sign Language Recognition (SLR) aims to learn spatial and temporal representations from the videos of the signs. Most SLR studies focus on manual features often extracted from the shape of the dominant hand or the entire frame. However, facial expressions combined with hand and body gestures may also play a significant role in discriminating the context represented in the sign videos. In this study, we propose an isolated SLR framework based on Spatial-Temporal Graph Convolutional Networks (ST-GCNs) and Multi-Cue Long Short-Term Memorys (MC-LSTMs) to exploit multi-articulatory (e.g., body, hands, and face) information for recognizing sign glosses. We train an ST-GCN model for learning representations from the upper body and hands. Meanwhile, spatial embeddings of hand shape and facial expression cues are extracted from Convolutional Neural Networks (CNNs) pre-trained on large-scale hand and facial expression datasets. Thus, the proposed framework coupling ST-GCNs with MC-LSTMs for multi-articulatory temporal modeling can provide insights into the contribution of each visual Sign Language (SL) cue to recognition performance. To evaluate the proposed framework, we conducted extensive analyzes on two Turkish SL benchmark datasets with different linguistic properties, BosphorusSign22k and AUTSL. While we obtained comparable recognition performance with the skeleton-based state-of-the-art, we observe that incorporating multiple visual SL cues improves the recognition performance, especially in certain sign classes where multi-cue information is vital. The code is available at:
              https://github.com/ogulcanozdemir/multicue-slr
              .},
  urldate  = {2024-02-19},
  journal  = {Frontiers in Neuroscience},
  author   = {Özdemir, Oğulcan and Baytaş, İnci M. and Akarun, Lale},
  month    = apr,
  year     = {2023},
  pages    = {1148191}
}

@inproceedings{dafnis_bidirectional_2022,
  title    = {Bidirectional {Skeleton}-{Based} {Isolated} {Sign} {Recognition} using {Graph} {Convolutional} {Networks}},
  url      = {https://www.semanticscholar.org/paper/Bidirectional-Skeleton-Based-Isolated-Sign-using-Dafnis-Chroni/6ce0b2b205d3dd96d4930c44884d6b03e04a0f67},
  abstract = {To improve computer-based recognition from video of isolated signs from American Sign Language (ASL), we propose a new skeleton-based method that involves explicit detection of the start and end frames of signs, trained on the ASLLVD dataset; it uses linguistically relevant parameters based on the skeleton input. Our method employs a bidirectional learning approach within a Graph Convolutional Network (GCN) framework. We apply this method to the WLASL dataset, but with corrections to the gloss labeling to ensure consistency in the labels assigned to different signs; it is important to have a 1-1 correspondence between signs and text-based gloss labels. We achieve a success rate of 77.43\% for top-1 and 94.54\% for top-5 using this modified WLASL dataset. Our method, which does not require multi-modal data input, outperforms other state-of-the-art approaches on the same modified WLASL dataset, demonstrating the importance of both attention to the start and end frames of signs and the use of bidirectional data streams in the GCNs for isolated sign recognition.},
  urldate  = {2024-02-19},
  author   = {Dafnis, Konstantinos M. and Chroni, Evgenia and Neidle, C. and Metaxas, Dimitris N.},
  year     = {2022}
}

@inproceedings{vazquez-enriquez_isolated_2021,
  address   = {Nashville, TN, USA},
  title     = {Isolated {Sign} {Language} {Recognition} with {Multi}-{Scale} {Spatial}-{Temporal} {Graph} {Convolutional} {Networks}},
  isbn      = {978-1-66544-899-4},
  url       = {https://ieeexplore.ieee.org/document/9523152/},
  doi       = {10.1109/CVPRW53098.2021.00385},
  urldate   = {2024-02-19},
  booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
  publisher = {IEEE},
  author    = {Vazquez-Enriquez, Manuel and Alba-Castro, Jose L. and Docio-Fernandez, Laura and Rodriguez-Banga, Eduardo},
  month     = jun,
  year      = {2021},
  pages     = {3457--3466}
}

@article{sanchez_ruiz_word_2023,
  title    = {Word {Level} {Sign} {Language} {Recognition} via {Handcrafted} {Features}},
  volume   = {21},
  issn     = {1548-0992},
  url      = {https://ieeexplore.ieee.org/document/10244183/},
  doi      = {10.1109/TLA.2023.10244183},
  abstract = {The ability to be understood and convey feelings, requests or ideas through words (spoken or written) is one of the most undervalue by all the humans who have the privilege to do it. Deaf community faces this challenge every single day and, even though, sign languages exist as way to battle against this issue, not all in deaf community knows who to use them; in fact, hearing community knows in a smaller proportion how to interpret them. By this reason sign language recognition area becomes relevant as an effort to solve this issue and create new communication channels.},
  language = {en},
  number   = {7},
  urldate  = {2024-02-19},
  journal  = {IEEE Latin America Transactions},
  author   = {Sánchez Ruiz, Daniel and Olvera-López, J. Arturo and Olmos-Pineda, Ivan},
  month    = jul,
  year     = {2023},
  pages    = {839--848}
}

@article{das_automated_2023,
  title    = {Automated {Indian} sign language recognition system by fusing deep and handcrafted feature},
  volume   = {82},
  issn     = {1380-7501, 1573-7721},
  url      = {https://link.springer.com/10.1007/s11042-022-14084-4},
  doi      = {10.1007/s11042-022-14084-4},
  abstract = {The deaf community faces some major challenges due to the communication gap with the hearing community. The traditional approach of employing a Sign Language (SL) interpreter is not an efficient and cost-effective solution to this problem. Thus, an automated Sign Language Recognition System (SLRS) is needed to provide an efficient and reliable solution. Existing SLRS for dynamic SL recognition utilizes the CNN-LSTM architecture, which has accomplished satisfactory results. However, spatial features extracted through Convolutional Neural Network (CNN) are insufficient for recognizing SL word that consists of identical hand orientation and multiple viewing angles. Thus, this paper proposes an SLRS named Automated Indian Sign Language Recognition System for Emergency Words (AISLRSEW) for recognizing ISL words which are frequently used in an emergency situation. The proposed AISLRSEW uses a combination of CNN and local handcrafted features to resolve the issue of identifying SL words with identical hand orientation and multiple viewing angles, which improves the recognition accuracy. The performance of the proposed AISLRSEW is evaluated with two fold cross-validation method and compared with existing models. The proposed model has achieved an average accuracy of 94.42\%, which is comparatively better than existing models.},
  language = {en},
  number   = {11},
  urldate  = {2024-02-19},
  journal  = {Multimedia Tools and Applications},
  author   = {Das, Soumen and Biswas, Saroj Kr and Purkayastha, Biswajit},
  month    = may,
  year     = {2023},
  pages    = {16905--16927}
}

@inproceedings{jie_huang_sign_2015,
  address   = {Turin, Italy},
  title     = {Sign {Language} {Recognition} using {3D} convolutional neural networks},
  isbn      = {978-1-4799-7082-7},
  url       = {https://ieeexplore.ieee.org/document/7177428},
  doi       = {10.1109/ICME.2015.7177428},
  abstract  = {Sign Language Recognition (SLR) targets on interpreting the sign language into text or speech, so as to facilitate the communication between deaf-mute people and ordinary people. This task has broad social impact, but is still very challenging due to the complexity and large variations in hand actions. Existing methods for SLR use hand-crafted features to describe sign language motion and build classiﬁcation models based on those features. However, it is difﬁcult to design reliable features to adapt to the large variations of hand gestures. To approach this problem, we propose a novel 3D convolutional neural network (CNN) which extracts discriminative spatial-temporal features from raw video stream automatically without any prior knowledge, avoiding designing features. To boost the performance, multi-channels of video streams, including color information, depth clue, and body joint positions, are used as input to the 3D CNN in order to integrate color, depth and trajectory information. We validate the proposed model on a real dataset collected with Microsoft Kinect and demonstrate its effectiveness over the traditional approaches based on hand-crafted features.},
  language  = {en},
  urldate   = {2024-02-19},
  booktitle = {2015 {IEEE} {International} {Conference} on {Multimedia} and {Expo} ({ICME})},
  publisher = {IEEE},
  author    = {{Jie Huang} and {Wengang Zhou} and {Houqiang Li} and {Weiping Li}},
  month     = jun,
  year      = {2015},
  pages     = {1--6}
}

@misc{hu_squeeze-and-excitation_2019,
  title     = {Squeeze-and-{Excitation} {Networks}},
  url       = {http://arxiv.org/abs/1709.01507},
  abstract  = {The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive ﬁelds at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the “Squeeze-and-Excitation” (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring signiﬁcant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classiﬁcation submission which won ﬁrst place and reduced the top-5 error to 2.251\%, surpassing the winning entry of 2016 by a relative improvement of ∼25\%. Models and code are available at https://github.com/hujie-frank/SENet.},
  language  = {en},
  urldate   = {2024-02-19},
  publisher = {arXiv},
  author    = {Hu, Jie and Shen, Li and Albanie, Samuel and Sun, Gang and Wu, Enhua},
  month     = may,
  year      = {2019},
  note      = {arXiv:1709.01507 [cs]},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{liu_skepxels_2018,
  title      = {Skepxels: {Spatio}-temporal {Image} {Representation} of {Human} {Skeleton} {Joints} for {Action} {Recognition}},
  shorttitle = {Skepxels},
  url        = {http://arxiv.org/abs/1711.05941},
  abstract   = {Human skeleton joints are popular for action analysis since they can be easily extracted from videos to discard background noises. However, current skeleton representations do not fully beneﬁt from machine learning with Convolutional Neural Networks (CNNs). We propose “Skepxels” a spatiotemporal representation for skeleton sequences to fully exploit the correlations between joints using the kernels of CNNs. We transform skeleton videos into images of ﬂexible dimensions using Skepxels and develop a CNN-based framework for effective human action recognition using the resulting images. Skepxels encode rich spatio-temporal information about the skeleton joints in the frames by maximizing a unique distance metric, deﬁned collaboratively over the distinct joint arrangements used in the skeletal images. Moreover, they are ﬂexible in encoding compound semantic notions such as location and speed of the joints. The proposed action recognition exploits the representation in a hierarchical manner by ﬁrst capturing the micro-temporal relations between the skeleton joints with the Skepxels and then exploiting their macro-temporal relations by computing the Fourier Temporal Pyramids over the CNN features of the skeletal images. We extend the Inception-ResNet CNN architecture with the proposed method and improve the state-of-the-art accuracy by 4.4\% on the large scale NTU human activity dataset. On the medium-sized NUCLA and UTD-MHAD datasets, our method outperforms the existing results by 5.7\% and 9.3\% respectively.},
  language   = {en},
  urldate    = {2024-02-19},
  publisher  = {arXiv},
  author     = {Liu, Jian and Akhtar, Naveed and Mian, Ajmal},
  month      = aug,
  year       = {2018},
  note       = {arXiv:1711.05941 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{hosain_sign_2019,
  title     = {Sign {Language} {Recognition} {Analysis} using {Multimodal} {Data}},
  url       = {http://arxiv.org/abs/1909.11232},
  abstract  = {Voice-controlled personal and home assistants (such as the Amazon Echo and Apple Siri) are becoming increasingly popular for a variety of applications. However, the benefits of these technologies are not readily accessible to Deaf or Hard-ofHearing (DHH) users. The objective of this study is to develop and evaluate a sign recognition system using multiple modalities that can be used by DHH signers to interact with voice-controlled devices. With the advancement of depth sensors, skeletal data is used for applications like video analysis and activity recognition. Despite having similarity with the well-studied human activity recognition, the use of 3D skeleton data in sign language recognition is rare. This is because unlike activity recognition, sign language is mostly dependent on hand shape pattern. In this work, we investigate the feasibility of using skeletal and RGB video data for sign language recognition using a combination of different deep learning architectures. We validate our results on a large-scale American Sign Language (ASL) dataset of 12 users and 13107 samples across 51 signs. It is named as GMUASL51. We collected the dataset over 6 months and it will be publicly released in the hope of spurring further machine learning research towards providing improved accessibility for digital assistants.},
  language  = {en},
  urldate   = {2024-02-19},
  publisher = {arXiv},
  author    = {Hosain, Al Amin and Santhalingam, Panneer Selvam and Pathak, Parth and Kosecka, Jana and Rangwala, Huzefa},
  month     = sep,
  year      = {2019},
  note      = {arXiv:1909.11232 [cs, stat]},
  keywords  = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{das_spatio-temporal_nodate,
  title    = {Spatio-temporal {Attention} {Mechanisms} for {Activity} {Recognition}},
  language = {en},
  author   = {Das, Srijan}
}

@misc{hosain_finehand_2020,
  title      = {{FineHand}: {Learning} {Hand} {Shapes} for {American} {Sign} {Language} {Recognition}},
  shorttitle = {{FineHand}},
  url        = {http://arxiv.org/abs/2003.08753},
  abstract   = {American Sign Language recognition is a difﬁcult gesture recognition problem, characterized by fast, highly articulate gestures. These are comprised of arm movements with different hand shapes, facial expression and head movements. Among these components, hand shape is the vital, often the most discriminative part of a gesture. In this work, we present an approach for effective learning of hand shape embeddings, which are discriminative for ASL gestures. For hand shape recognition our method uses a mix of manually labelled hand shapes and high conﬁdence predictions to train deep convolutional neural network (CNN). The sequential gesture component is captured by recursive neural network (RNN) trained on the embeddings learned in the ﬁrst stage. We will demonstrate that higher quality hand shape models can signiﬁcantly improve the accuracy of ﬁnal video gesture classiﬁcation in challenging conditions with variety of speakers, different illumination and signiﬁcant motion blurr. We compare our model to alternative approaches exploiting different modalities and representations of the data and show improved video gesture recognition accuracy on GMU-ASL51 benchmark dataset.},
  language   = {en},
  urldate    = {2024-02-19},
  publisher  = {arXiv},
  author     = {Hosain, Al Amin and Santhalingam, Panneer Selvam and Pathak, Parth and Rangwala, Huzefa and Kosecka, Jana},
  month      = mar,
  year       = {2020},
  note       = {arXiv:2003.08753 [cs, stat]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@inproceedings{hu_signbert_2021,
  address    = {Montreal, QC, Canada},
  title      = {{SignBERT}: {Pre}-{Training} of {Hand}-{Model}-{Aware} {Representation} for {Sign} {Language} {Recognition}},
  isbn       = {978-1-66542-812-5},
  shorttitle = {{SignBERT}},
  url        = {https://ieeexplore.ieee.org/document/9709967/},
  doi        = {10.1109/ICCV48922.2021.01090},
  abstract   = {Hand gesture serves as a critical role in sign language. Current deep-learning-based sign language recognition (SLR) methods may suffer insufficient interpretability and overfitting due to limited sign data sources. In this paper, we introduce the first self-supervised pre-trainable SignBERT with incorporated hand prior for SLR. SignBERT views the hand pose as a visual token, which is derived from an off-the-shelf pose extractor. The visual tokens are then embedded with gesture state, temporal and hand chirality information. To take full advantage of available sign data sources, SignBERT first performs self-supervised pre-training by masking and reconstructing visual tokens. Jointly with several mask modeling strategies, we attempt to incorporate hand prior in a model-aware method to better model hierarchical context over the hand sequence. Then with the prediction head added, SignBERT is fine-tuned to perform the downstream SLR task. To validate the effectiveness of our method on SLR, we perform extensive experiments on four public benchmark datasets, i.e., NMFs-CSL, SLR500, MSASL and WLASL. Experiment results demonstrate the effectiveness of both self-supervised learning and imported hand prior. Furthermore, we achieve state-of-theart performance on all benchmarks with a notable gain.},
  language   = {en},
  urldate    = {2024-02-19},
  booktitle  = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
  publisher  = {IEEE},
  author     = {Hu, Hezhen and Zhao, Weichao and Zhou, Wengang and Wang, Yuechen and Li, Houqiang},
  month      = oct,
  year       = {2021},
  pages      = {11067--11076}
}

@article{shabaninia_multimodal_2023,
  title      = {Multimodal action recognition: a comprehensive survey on temporal modeling},
  issn       = {1380-7501, 1573-7721},
  shorttitle = {Multimodal action recognition},
  url        = {https://link.springer.com/10.1007/s11042-023-17345-y},
  doi        = {10.1007/s11042-023-17345-y},
  abstract   = {In action recognition that relies on visual information, activities are recognized through spatio-temporal features from different modalities. The challenge of temporal modeling has been a long-standing issue in this ﬁeld. There are a limited number of methods, such as pre-computed motion features, three-dimensional (3D) ﬁlters, and recurrent neural networks (RNNs), that are used in deep-based approaches to model motion information. However, the success of transformers in modeling long-range dependencies in natural language processing tasks has recently caught the attention of other domains, including speech, image, and video, as they can rely entirely on self-attention without using sequence-aligned RNNs or convolutions. Although the application of transformers to action recognition is relatively new, the amount of research proposed on this topic in the last few years is impressive. This paper aims to review recent progress in deep learning methods for modeling temporal variations in multimodal human action recognition. Speciﬁcally, it focuses on methods that use transformers for temporal modeling, highlighting their key features and the modalities they employ, while also identifying opportunities and challenges for future research.},
  language   = {en},
  urldate    = {2024-02-19},
  journal    = {Multimedia Tools and Applications},
  author     = {Shabaninia, Elham and Nezamabadi-pour, Hossein and Shafizadegan, Fatemeh},
  month      = dec,
  year       = {2023}
}

@misc{maruyama_word-level_2021,
  title     = {Word-level {Sign} {Language} {Recognition} with {Multi}-stream {Neural} {Networks} {Focusing} on {Local} {Regions}},
  url       = {http://arxiv.org/abs/2106.15989},
  abstract  = {In recent years, Word-level Sign Language Recognition (WSLR) research has gained popularity in the computer vision community, and thus various approaches have been proposed. Among these approaches, the method using I3D network achieves the highest recognition accuracy on large public datasets for WSLR. However, the method with I3D only utilizes appearance information of the upper body of the signers to recognize sign language words. On the other hand, in WSLR, the information of local regions, such as the hand shape and facial expression, and the positional relationship among the body and both hands are important. Thus in this work, we utilized local region images of both hands and face, along with skeletal information to capture local information and the positions of both hands relative to the body, respectively. In other words, we propose a novel multi-stream WSLR framework, in which a stream with local region images and a stream with skeletal information are introduced by extending I3D network to improve the recognition accuracy of WSLR. From the experimental results on WLASL dataset, it is evident that the proposed method has achieved about 15\% improvement in the Top-1 accuracy than the existing conventional methods.},
  language  = {en},
  urldate   = {2024-02-19},
  publisher = {arXiv},
  author    = {Maruyama, Mizuki and Ghose, Shuvozit and Inoue, Katsufumi and Roy, Partha Pratim and Iwamura, Masakazu and Yoshioka, Michifumi},
  month     = jun,
  year      = {2021},
  note      = {arXiv:2106.15989 [cs]},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Multimedia}
}

@misc{li_word_level_WLASL_2020,
  title      = {Word-level {Deep} {Sign} {Language} {Recognition} from {Video}: {A} {New} {Large}-scale {Dataset} and {Methods} {Comparison}},
  shorttitle = {Word-level {Deep} {Sign} {Language} {Recognition} from {Video}},
  url        = {http://arxiv.org/abs/1910.11006},
  abstract   = {Vision-based sign language recognition aims at helping deaf people to communicate with others. However, most existing sign language datasets are limited to a small number of words. Due to the limited vocabulary size, models learned from those datasets cannot be applied in practice. In this paper, we introduce a new large-scale Word-Level American Sign Language (WLASL) video dataset, containing more than 2000 words performed by over 100 signers. This dataset will be made publicly available to the research community. To our knowledge,it is by far the largest public ASL dataset to facilitate word-level sign recognition research.},
  language   = {en},
  urldate    = {2024-02-19},
  publisher  = {arXiv},
  author     = {Li, Dongxu and Opazo, Cristian Rodriguez and Yu, Xin and Li, Hongdong},
  month      = jan,
  year       = {2020},
  note       = {arXiv:1910.11006 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction, Computer Science - Multimedia, Computer Science - Neural and Evolutionary Computing}
}

@article{sincan_using_2022,
  title    = {Using {Motion} {History} {Images} with {3D} {Convolutional} {Networks} in {Isolated} {Sign} {Language} {Recognition}},
  volume   = {10},
  issn     = {2169-3536},
  url      = {http://arxiv.org/abs/2110.12396},
  doi      = {10.1109/ACCESS.2022.3151362},
  abstract = {Sign language recognition using computational models is a challenging problem that requires simultaneous spatio-temporal modeling of the multiple sources, i.e. faces, hands, body, etc. In this paper, we propose an isolated sign language recognition model based on a model trained using Motion History Images (MHI) that are generated from RGB video frames. RGB-MHI images represent spatio-temporal summary of each sign video eﬀectively in a single RGB image. We propose two diﬀerent approaches using this RGB-MHI model. In the ﬁrst approach, we use the RGB-MHI model as a motion-based spatial attention module integrated into a 3D-CNN architecture. In the second approach, we use RGB-MHI model features directly with the features of a 3D-CNN model using a late fusion technique. We perform extensive experiments on two recently released large-scale isolated sign language datasets, namely AUTSL and BosphorusSign22k. Our experiments show that our models, which use only RGB data, can compete with the state-of-the-art models in the literature that use multi-modal data.},
  language = {en},
  urldate  = {2024-02-19},
  journal  = {IEEE Access},
  author   = {Sincan, Ozge Mercanoglu and Keles, Hacer Yalim},
  year     = {2022},
  note     = {arXiv:2110.12396 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  pages    = {18608--18618}
}

@article{li_spatio-temporal_2020,
  title    = {Spatio-{Temporal} {Attention} {Networks} for {Action} {Recognition} and {Detection}},
  volume   = {22},
  issn     = {1520-9210, 1941-0077},
  url      = {https://ieeexplore.ieee.org/document/8955791/},
  doi      = {10.1109/TMM.2020.2965434},
  abstract = {Recently, 3D Convolutional Neural Network (3D CNN) models have been widely studied for video sequences and achieved satisfying performance in action recognition and detection tasks. However, most of the existing 3D CNNs treat all input video frames equally, thus ignoring the spatial and temporal differences across the video frames. To address the problem, we propose a spatio-temporal attention (STA) network that is able to learn the discriminative feature representation for actions, by respectively characterizing the beneﬁcial information at both the frame level and the channel level. By simultaneously exploiting the differences in spatial and temporal dimensions, our STA module enhances the learning capability of the 3D convolutions when handling the complex videos. The proposed STA method can be wrapped as a generic module easily plugged into the state-of-the-art 3D CNN architectures for video action detection and recognition. We extensively evaluate our method on action recognition and detection tasks over three popular datasets (UCF-101, HMDB-51 and THUMOS 2014), and the experimental results demonstrate that adding our STA network module can obtain the state-of-the-art performance on UCF-101 and HMDB-51, which has the top-1 accuracies of 98.4\% and 81.4\% respectively, and achieve signiﬁcant improvement on THUMOS 2014 dataset compared against original models.},
  language = {en},
  number   = {11},
  urldate  = {2024-02-19},
  journal  = {IEEE Transactions on Multimedia},
  author   = {Li, Jun and Liu, Xianglong and Zhang, Wenxuan and Zhang, Mingyuan and Song, Jingkuan and Sebe, Nicu},
  month    = nov,
  year     = {2020},
  pages    = {2990--3001}
}

@misc{li_skeleton-based_2017,
  title     = {Skeleton-based {Action} {Recognition} {Using} {LSTM} and {CNN}},
  url       = {http://arxiv.org/abs/1707.02356},
  abstract  = {Recent methods based on 3D skeleton data have achieved outstanding performance due to its conciseness, robustness, and view-independent representation. With the development of deep learning, Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM)-based learning methods have achieved promising performance for action recognition. However, for CNN-based methods, it is inevitable to loss temporal information when a sequence is encoded into images. In order to capture as much spatial-temporal information as possible, LSTM and CNN are adopted to conduct effective recognition with later score fusion. In addition, experimental results show that the score fusion between CNN and LSTM performs better than that between LSTM and LSTM for the same feature. Our method achieved state-of-the-art results on NTU RGB+D datasets for 3D human action analysis. The proposed method achieved 87.40\% in terms of accuracy and ranked 1st place in Large Scale 3D Human Activity Analysis Challenge in Depth Videos.},
  language  = {en},
  urldate   = {2024-02-19},
  publisher = {arXiv},
  author    = {Li, Chuankun and Wang, Pichao and Wang, Shuang and Hou, Yonghong and Li, Wanqing},
  month     = jul,
  year      = {2017},
  note      = {arXiv:1707.02356 [cs]},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{jiang_skeleton_2021,
  title     = {Skeleton {Aware} {Multi}-modal {Sign} {Language} {Recognition}},
  url       = {http://arxiv.org/abs/2103.08833},
  abstract  = {Sign language is commonly used by deaf or speech impaired people to communicate but requires significant effort to master. Sign Language Recognition (SLR) aims to bridge the gap between sign language users and others by recognizing signs from given videos. It is an essential yet challenging task since sign language is performed with the fast and complex movement of hand gestures, body posture, and even facial expressions. Recently, skeleton-based action recognition attracts increasing attention due to the independence between the subject and background variation. However, skeleton-based SLR is still under exploration due to the lack of annotations on hand keypoints. Some efforts have been made to use hand detectors with pose estimators to extract hand key points and learn to recognize sign language via Neural Networks, but none of them outperforms RGB-based methods. To this end, we propose a novel Skeleton Aware Multi-modal SLR framework (SAM-SLR) to take advantage of multi-modal information towards a higher recognition rate. Specifically, we propose a Sign Language Graph Convolution Network (SL-GCN) to model the embedded dynamics and a novel Separable Spatial-Temporal Convolution Network (SSTCN) to exploit skeleton features. RGB and depth modalities are also incorporated and assembled into our framework to provide global information that is complementary to the skeleton-based methods SL-GCN and SSTCN. As a result, SAM-SLR achieves the highest performance in both RGB (98.42{\textbackslash}\%) and RGB-D (98.53{\textbackslash}\%) tracks in 2021 Looking at People Large Scale Signer Independent Isolated SLR Challenge. Our code is available at https://github.com/jackyjsy/CVPR21Chal-SLR},
  language  = {en},
  urldate   = {2024-02-19},
  publisher = {arXiv},
  author    = {Jiang, Songyao and Sun, Bin and Wang, Lichen and Bai, Yue and Li, Kunpeng and Fu, Yun},
  month     = may,
  year      = {2021},
  note      = {arXiv:2103.08833 [cs]},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{carreira_quo_I3D_2018,
  title      = {Quo {Vadis}, {Action} {Recognition}? {A} {New} {Model} and the {Kinetics} {Dataset}},
  shorttitle = {Quo {Vadis}, {Action} {Recognition}?},
  url        = {http://arxiv.org/abs/1705.07750},
  abstract   = {The paucity of videos in current action classiﬁcation datasets (UCF-101 and HMDB-51) has made it difﬁcult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classiﬁcation on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics.},
  language   = {en},
  urldate    = {2024-02-19},
  publisher  = {arXiv},
  author     = {Carreira, Joao and Zisserman, Andrew},
  month      = feb,
  year       = {2018},
  note       = {arXiv:1705.07750 [cs]},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning}
}

@misc{kratimenos_independent_2020,
  title     = {Independent {Sign} {Language} {Recognition} with {3D} {Body}, {Hands}, and {Face} {Reconstruction}},
  url       = {http://arxiv.org/abs/2012.05698},
  abstract  = {Independent Sign Language Recognition is a complex visual recognition problem that combines several challenging tasks of Computer Vision due to the necessity to exploit and fuse information from hand gestures, body features and facial expressions. While many state-of-the-art works have managed to deeply elaborate on these features independently, to the best of our knowledge, no work has adequately combined all three information channels to efﬁciently recognize Sign Language. In this work, we employ SMPL-X, a contemporary parametric model that enables joint extraction of 3D body shape, face and hands information from a single image. We use this holistic 3D reconstruction for SLR, demonstrating that it leads to higher accuracy than recognition from raw RGB images and their optical ﬂow fed into the state-of-the-art I3D-type network for 3D action recognition and from 2D Openpose skeletons fed into a Recurrent Neural Network. Finally, a set of experiments on the body, face and hand features showed that neglecting any of these, significantly reduces the classiﬁcation accuracy, proving the importance of jointly modeling body shape, facial expression and hand pose for Sign Language Recognition.},
  language  = {en},
  urldate   = {2024-02-19},
  publisher = {arXiv},
  author    = {Kratimenos, Agelos and Pavlakos, Georgios and Maragos, Petros},
  month     = nov,
  year      = {2020},
  note      = {arXiv:2012.05698 [cs]},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition}
}

@misc{lee_human_2023,
  title     = {Human {Part}-wise {3D} {Motion} {Context} {Learning} for {Sign} {Language} {Recognition}},
  url       = {http://arxiv.org/abs/2308.09305},
  abstract  = {In this paper, we propose P3D, the human part-wise motion context learning framework for sign language recognition. Our main contributions lie in two dimensions: learning the part-wise motion context and employing the pose ensemble to utilize 2D and 3D pose jointly. First, our empirical observation implies that part-wise context encoding benefits the performance of sign language recognition. While previous methods of sign language recognition learned motion context from the sequence of the entire pose, we argue that such methods cannot exploit part-specific motion context. In order to utilize part-wise motion context, we propose the alternating combination of a part-wise encoding Transformer (PET) and a whole-body encoding Transformer (WET). PET encodes the motion contexts from a part sequence, while WET merges them into a unified context. By learning part-wise motion context, our P3D achieves superior performance on WLASL compared to previous stateof-the-art methods. Second, our framework is the first to ensemble 2D and 3D poses for sign language recognition. Since the 3D pose holds rich motion context and depth information to distinguish the words, our P3D outperformed the previous state-of-the-art methods employing a pose ensemble.},
  language  = {en},
  urldate   = {2024-02-19},
  publisher = {arXiv},
  author    = {Lee, Taeryung and Oh, Yeonguk and Lee, Kyoung Mu},
  month     = aug,
  year      = {2023},
  note      = {arXiv:2308.09305 [cs]},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition}
}

@inproceedings{hosain_hand_2021,
  address   = {Waikoloa, HI, USA},
  title     = {Hand {Pose} {Guided} {3D} {Pooling} for {Word}-level {Sign} {Language} {Recognition}},
  isbn      = {978-1-66540-477-8},
  url       = {https://ieeexplore.ieee.org/document/9423424/},
  doi       = {10.1109/WACV48630.2021.00347},
  abstract  = {Gestures in American Sign Language (ASL) are characterized by fast, highly articulate motion of upper body, including arm movements with complex hand shapes and facial expressions. In this work, we propose a new method for word-level sign recognition from American Sign Language (ASL) using video. Our method uses both motion and hand shape cues while being robust to variations of execution. We exploit the knowledge of the body pose, estimated from an off-the-shelf pose estimator. Using the pose as a guide, we pool spatio-temporal feature maps from different layers of a 3D convolutional neural network. We train separate classiﬁers using pose guided pooled features from different resolutions and fuse their prediction scores during test time. This leads to a signiﬁcant improvement in performance on the WLASL benchmark dataset [25]. The proposed approach achieves 10\%, 12\%, 9.5\% and 6.5\% performance gain on WLASL100, WLASL300, WLASL1000, WLASL2000 subsets respectively. To demonstrate the robustness of the pose guided pooling and proposed fusion mechanism, we also evaluate our method by ﬁne tuning the model on another dataset. This yields 10\% performance improvement for the proposed method using only 0.4\% training data during ﬁne tuning stage.},
  language  = {en},
  urldate   = {2024-02-19},
  booktitle = {2021 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
  publisher = {IEEE},
  author    = {Hosain, Al Amin and Selvam Santhalingam, Panneer and Pathak, Parth and Rangwala, Huzefa and Kosecka, Jana},
  month     = jan,
  year      = {2021},
  pages     = {3428--3438}
}

@misc{caetano_skeleton_2019,
  title     = {Skeleton {Image} {Representation} for {3D} {Action} {Recognition} based on {Tree} {Structure} and {Reference} {Joints}},
  url       = {http://arxiv.org/abs/1909.05704},
  abstract  = {In the last years, the computer vision research community has studied on how to model temporal dynamics in videos to employ 3D human action recognition. To that end, two main baseline approaches have been researched: (i) Recurrent Neural Networks (RNNs) with Long-Short Term Memory (LSTM); and (ii) skeleton image representations used as input to a Convolutional Neural Network (CNN). Although RNN approaches present excellent results, such methods lack the ability to efﬁciently learn the spatial relations between the skeleton joints. On the other hand, the representations used to feed CNN approaches present the advantage of having the natural ability of learning structural information from 2D arrays (i.e., they learn spatial relations from the skeleton joints). To further improve such representations, we introduce the Tree Structure Reference Joints Image (TSRJI), a novel skeleton image representation to be used as input to CNNs. The proposed representation has the advantage of combining the use of reference joints and a tree structure skeleton. While the former incorporates different spatial relationships between the joints, the latter preserves important spatial relations by traversing a skeleton tree with a depth-ﬁrst order algorithm. Experimental results demonstrate the effectiveness of the proposed representation for 3D action recognition on two datasets achieving state-of-the-art results on the recent NTU RGB+D 120 dataset.},
  language  = {en},
  urldate   = {2024-02-19},
  publisher = {arXiv},
  author    = {Caetano, Carlos and Brémond, François and Schwartz, William Robson},
  month     = sep,
  year      = {2019},
  note      = {arXiv:1909.05704 [cs]},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning}
}

@misc{laines_isolated_2023,
  title     = {Isolated {Sign} {Language} {Recognition} based on {Tree} {Structure} {Skeleton} {Images}},
  url       = {http://arxiv.org/abs/2304.05403},
  abstract  = {Sign Language Recognition (SLR) systems aim to be embedded in video stream platforms to recognize the sign performed in front of a camera. SLR research has taken advantage of recent advances in pose estimation models to use skeleton sequences estimated from videos instead of RGB information to predict signs. This approach can make HARrelated tasks less complex and more robust to diverse backgrounds, lightning conditions, and physical appearances. In this work, we explore the use of a spatio-temporal skeleton representation such as Tree Structure Skeleton Image (TSSI) as an alternative input to improve the accuracy of skeleton-based models for SLR. TSSI converts a skeleton sequence into an RGB image where the columns represent the joints of the skeleton in a depth-ﬁrst tree traversal order, the rows represent the temporal evolution of the joints, and the three channels represent the (x, y, z) coordinates of the joints. We trained a DenseNet-121 using this type of input and compared it with other skeleton-based deep learning methods using a large-scale American Sign Language (ASL) dataset, WLASL. Our model (SL-TSSI-DenseNet) overcomes the state-of-the-art of other skeleton-based models. Moreover, when including data augmentation our proposal achieves better results than both skeleton-based and RGB-based models. We evaluated the effectiveness of our model on the Ankara University Turkish Sign Language (TSL) dataset, AUTSL, and a Mexican Sign Language (LSM) dataset. On the AUTSL dataset, the model achieves similar results to the state-of-the-art of other skeleton-based models. On the LSM dataset, the model achieves higher results than the baseline. As far as we know, our work is the ﬁrst to try TSSI for sign language recognition and our results suggest it presents a real alternative for isolated sign language representation. Code has been made available at: https://github.com/davidlainesv/SL-TSSI-DenseNet.},
  language  = {en},
  urldate   = {2024-02-19},
  publisher = {arXiv},
  author    = {Laines, David and Bejarano, Gissella and Gonzalez-Mendoza, Miguel and Ochoa-Ruiz, Gilberto},
  month     = apr,
  year      = {2023},
  note      = {arXiv:2304.05403 [cs]},
  keywords  = {Computer Science - Computer Vision and Pattern Recognition}
}


@inproceedings{vakunov2020mediapipe_hands,
  title     = {Mediapipe hands: On-device real-time hand tracking},
  author    = {Vakunov, Andrey and Chang, Chuo-Ling and Zhang, Fan and Sung, George and Grundmann, Matthias and Bazarevsky, Valentin},
  booktitle = {Workshop on Computer Vision for AR/VR},
  url       = {https://arxiv.org/pdf/2006.10214},
  volume    = {2},
  number    = {4},
  pages     = {5},
  year      = {2020}
}


@article{Bazarevsky2020BlazePoseOR,
  title   = {BlazePose: On-device Real-time Body Pose tracking},
  author  = {Valentin Bazarevsky and Ivan Grishchenko and Karthik Raveendran and Tyler Lixuan Zhu and Fan Zhang and Matthias Grundmann},
  journal = {ArXiv},
  year    = {2020},
  volume  = {abs/2006.10204},
  url     = {https://api.semanticscholar.org/CorpusID:219793039}
}

@article{bazarevsky2019blazeface,
  title   = {Blazeface: Sub-millisecond neural face detection on mobile gpus},
  author  = {Bazarevsky, Valentin and Kartynnik, Yury and Vakunov, Andrey and Raveendran, Karthik and Grundmann, Matthias},
  journal = {arXiv preprint arXiv:1907.05047},
  year    = {2019}
}

@inproceedings{sandler2018mobilenetv2,
  title     = {Mobilenetv2: Inverted residuals and linear bottlenecks},
  author    = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {4510--4520},
  year      = {2018}
}

@article{Hearst_SVM_1998,
  author   = {Hearst, M.A. and Dumais, S.T. and Osuna, E. and Platt, J. and Scholkopf, B.},
  journal  = {IEEE Intelligent Systems and their Applications},
  title    = {Support vector machines},
  year     = {1998},
  volume   = {13},
  number   = {4},
  pages    = {18-28},
  keywords = {Support vector machines;Machine learning;Algorithm design and analysis;Pattern recognition;Neural networks;Training data;Polynomials;Kernel;Character recognition;Web pages},
  doi      = {10.1109/5254.708428}
}

@article{Cover_KNN_1967,
  author   = {Cover, T. and Hart, P.},
  journal  = {IEEE Transactions on Information Theory},
  title    = {Nearest neighbor pattern classification},
  year     = {1967},
  volume   = {13},
  number   = {1},
  pages    = {21-27},
  keywords = {},
  doi      = {10.1109/TIT.1967.1053964}
}

@inproceedings{bohavcek2022sign,
  title     = {Sign pose-based transformer for word-level sign language recognition},
  author    = {Boh{\'a}{\v{c}}ek, Maty{\'a}{\v{s}} and Hr{\'u}z, Marek},
  booktitle = {Proceedings of the IEEE/CVF winter conference on applications of computer vision},
  pages     = {182--191},
  year      = {2022}
}

@inproceedings{Aparna2020CNNAS,
  title  = {CNN and Stacked LSTM Model for Indian Sign Language Recognition},
  author = {Ch Aparna and M. Kalaiselvi Geetha},
  year   = {2020},
  url    = {https://api.semanticscholar.org/CorpusID:216512179}
}